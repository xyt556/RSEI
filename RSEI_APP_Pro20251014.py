import streamlit as st
from pathlib import Path
import numpy as np
import rasterio
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from matplotlib.patches import Patch
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd
import warnings
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass, field
import time
import io
import zipfile
import tempfile
import shutil

warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="RSEIè®¡ç®—ç³»ç»Ÿ",
    page_icon="ğŸŒ¿",
    layout="wide",
    initial_sidebar_state="expanded"
)


# =============================
# æ ¸å¿ƒè®¡ç®—ç±»ï¼ˆä¿æŒä¸å˜ï¼‰
# =============================
class JenksNaturalBreaks:
    """ä¼˜åŒ–çš„Jenksè‡ªç„¶é—´æ–­ç‚¹åˆ†ç±»ç®—æ³•"""

    @staticmethod
    def calculate_jenks_breaks(data: np.ndarray, n_classes: int = 5,
                               max_samples: int = 5000) -> List[float]:
        valid_data = data[~np.isnan(data)].flatten()
        if len(valid_data) == 0:
            raise ValueError("æ•°æ®å…¨ä¸ºNaN")

        if len(valid_data) > max_samples:
            st.info(f"æ•°æ®é‡ {len(valid_data):,} è¿‡å¤§ï¼Œé‡‡æ ·è‡³ {max_samples:,} ä¸ªç‚¹...")
            valid_data = JenksNaturalBreaks._stratified_sample(valid_data, max_samples)

        methods = [
            ('jenkspy', JenksNaturalBreaks._jenks_by_jenkspy),
            ('numba', JenksNaturalBreaks._jenks_by_numba),
            ('optimized_numpy', JenksNaturalBreaks._jenks_optimized_numpy),
            ('fallback', JenksNaturalBreaks._jenks_fallback)
        ]

        for method_name, method_func in methods:
            try:
                breaks = method_func(valid_data, n_classes)
                if breaks is not None:
                    st.success(f"ä½¿ç”¨æ–¹æ³•: {method_name}")
                    return breaks
            except Exception as e:
                if method_name != 'fallback':
                    st.warning(f"{method_name} æ–¹æ³•å¤±è´¥: {e}")
                    continue
                else:
                    raise

        return JenksNaturalBreaks.get_default_breaks()

    @staticmethod
    def _stratified_sample(data: np.ndarray, n_samples: int) -> np.ndarray:
        sorted_data = np.sort(data)
        indices = np.linspace(0, len(sorted_data) - 1, n_samples, dtype=int)
        np.random.seed(42)
        noise = np.random.randint(-2, 3, size=n_samples)
        indices = np.clip(indices + noise, 0, len(sorted_data) - 1)
        return sorted_data[indices]

    @staticmethod
    def _jenks_by_jenkspy(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        try:
            import jenkspy
            breaks = jenkspy.jenks_breaks(data, n_classes=n_classes)
            return breaks[1:-1]
        except ImportError:
            return None

    @staticmethod
    def _jenks_by_numba(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        try:
            from numba import jit

            @jit(nopython=True)
            def _jenks_matrices_numba(data, n_classes):
                n_data = len(data)
                mat1 = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)
                mat2 = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)

                for i in range(1, n_classes + 1):
                    mat1[1, i] = 1
                    mat2[1, i] = 0
                    for j in range(2, n_data + 1):
                        mat2[j, i] = np.inf

                for l in range(2, n_data + 1):
                    s1 = 0.0
                    s2 = 0.0

                    for m in range(1, l + 1):
                        i3 = l - m + 1
                        val = data[i3 - 1]
                        s2 += val * val
                        s1 += val
                        w = m
                        v = s2 - (s1 * s1) / w
                        i4 = i3 - 1

                        if i4 != 0:
                            for j in range(2, n_classes + 1):
                                if mat2[l, j] >= (v + mat2[i4, j - 1]):
                                    mat1[l, j] = i3
                                    mat2[l, j] = v + mat2[i4, j - 1]

                    mat1[l, 1] = 1
                    mat2[l, 1] = v

                return mat1, mat2

            sorted_data = np.sort(data)
            mat1, mat2 = _jenks_matrices_numba(sorted_data, n_classes)

            k = len(sorted_data)
            kclass = []

            for j in range(n_classes, 0, -1):
                idx = int(mat1[k, j]) - 2
                if idx >= 0 and idx < len(sorted_data):
                    kclass.append(sorted_data[idx])
                k = int(mat1[k, j]) - 1

            kclass.reverse()
            if len(kclass) > 1:
                return kclass[1:][:n_classes - 1]
            else:
                return None

        except ImportError:
            return None

    @staticmethod
    def _jenks_optimized_numpy(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        sorted_data = np.sort(data)
        n_data = len(sorted_data)

        lower_class_limits = np.zeros((n_data + 1, n_classes + 1), dtype=np.int32)
        variance_combinations = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)
        variance_combinations[:, :] = np.inf

        lower_class_limits[1:, 1] = 1
        variance_combinations[1, :] = 0

        for l in range(2, n_data + 1):
            sum_val = 0.0
            sum_sq = 0.0

            for m in range(1, l + 1):
                lower_class_limit = l - m + 1
                val = sorted_data[lower_class_limit - 1]

                sum_val += val
                sum_sq += val * val
                w = m

                variance = sum_sq - (sum_val * sum_val) / w

                if lower_class_limit > 1:
                    for j in range(2, n_classes + 1):
                        if variance_combinations[l, j] >= (
                                variance + variance_combinations[lower_class_limit - 1, j - 1]):
                            lower_class_limits[l, j] = lower_class_limit
                            variance_combinations[l, j] = variance + variance_combinations[
                                lower_class_limit - 1, j - 1]

            lower_class_limits[l, 1] = 1
            variance_combinations[l, 1] = variance

        k = n_data
        breaks = []

        for j in range(n_classes, 1, -1):
            idx = lower_class_limits[k, j] - 2
            if 0 <= idx < len(sorted_data):
                breaks.append(float(sorted_data[idx]))
            k = lower_class_limits[k, j] - 1

        breaks.reverse()

        if len(breaks) >= n_classes - 1:
            return breaks[:n_classes - 1]
        else:
            return None

    @staticmethod
    def _jenks_fallback(data: np.ndarray, n_classes: int) -> List[float]:
        sorted_data = np.sort(data)
        n = len(sorted_data)
        breaks = []

        quantiles = np.linspace(0, 1, n_classes + 1)[1:-1]

        for q in quantiles:
            idx = int(q * n)
            window = min(int(n * 0.05), 100)
            start = max(0, idx - window)
            end = min(n, idx + window)

            if start >= end - 1:
                breaks.append(sorted_data[idx])
                continue

            min_variance = np.inf
            best_idx = idx

            for i in range(start, end):
                if i == 0 or i == n:
                    continue

                left_var = np.var(sorted_data[max(0, i - window):i]) if i > window else 0
                right_var = np.var(sorted_data[i:min(n, i + window)]) if i < n - window else 0
                total_var = left_var + right_var

                if total_var < min_variance:
                    min_variance = total_var
                    best_idx = i

            breaks.append(float(sorted_data[best_idx]))

        return breaks[:n_classes - 1]

    @staticmethod
    def get_default_breaks() -> List[float]:
        return [0.2, 0.4, 0.6, 0.8]


@dataclass
class BandConfig:
    blue: int = 1
    green: int = 2
    red: int = 3
    nir: int = 4
    swir1: int = 5
    swir2: int = 6
    tir: int = 7

    @classmethod
    def landsat8_stacked(cls):
        return cls(blue=1, green=2, red=3, nir=4, swir1=5, swir2=6, tir=7)

    @classmethod
    def sentinel2_stacked(cls):
        return cls(blue=1, green=2, red=3, nir=4, swir1=5, swir2=6, tir=None)


@dataclass
class RSEIConfig:
    satellite: str = 'Landsat8'
    band_config: BandConfig = None
    use_pca: bool = True
    export_indices: bool = True
    export_geotiff: bool = True
    mask_water: bool = True
    water_index: str = 'MNDWI'
    water_threshold: Optional[float] = None
    use_otsu: bool = True
    use_jenks: bool = True
    classification_breaks: List[float] = field(default_factory=lambda: [0.2, 0.4, 0.6, 0.8])
    jenks_samples: int = 5000

    def __post_init__(self):
        if self.band_config is None:
            if self.satellite.lower() == 'landsat8':
                self.band_config = BandConfig.landsat8_stacked()
            elif self.satellite.lower() == 'sentinel2':
                self.band_config = BandConfig.sentinel2_stacked()
            else:
                self.band_config = BandConfig()


class MultiSpectralImageReader:
    def __init__(self, config: RSEIConfig):
        self.config = config
        self.metadata = None
        self.bands_data = {}

    def read_multiband_tif(self, tif_path: str) -> Dict[str, np.ndarray]:
        st.info(f"ğŸ“¡ è¯»å–å¤šæ³¢æ®µå½±åƒ: {Path(tif_path).name}")

        with rasterio.open(tif_path) as src:
            st.write(f"å°ºå¯¸: {src.width} x {src.height}, æ³¢æ®µæ•°: {src.count}")

            self.metadata = {
                'transform': src.transform,
                'crs': src.crs,
                'width': src.width,
                'height': src.height,
                'dtype': 'float32',
                'count': 1,
                'driver': 'GTiff',
                'nodata': np.nan
            }

            required_bands = self._get_required_bands()
            max_band_idx = max([idx for idx in required_bands.values() if idx is not None])

            if src.count < max_band_idx:
                raise ValueError(f"æ³¢æ®µæ•°é‡ä¸è¶³ï¼éœ€è¦{max_band_idx}ä¸ªï¼Œå®é™…{src.count}ä¸ª")

            bands = {}
            for band_name, band_idx in required_bands.items():
                if band_idx is None:
                    continue

                band_data = src.read(band_idx).astype(float)
                if src.nodata is not None:
                    band_data[band_data == src.nodata] = np.nan
                band_data[band_data < -9999] = np.nan
                band_data[band_data > 50000] = np.nan
                bands[band_name] = band_data

        st.success(f"âœ… æˆåŠŸè¯»å– {len(bands)} ä¸ªæ³¢æ®µ")
        self.bands_data = bands
        return bands

    def _get_required_bands(self) -> Dict[str, int]:
        bc = self.config.band_config
        return {
            'blue': bc.blue, 'green': bc.green, 'red': bc.red,
            'nir': bc.nir, 'swir1': bc.swir1, 'swir2': bc.swir2,
            'tir': bc.tir
        }

    def apply_scale_factor(self, bands: Dict[str, np.ndarray],
                           scale_factor: float = 0.0001) -> Dict[str, np.ndarray]:
        st.info(f"ğŸ”§ åº”ç”¨ç¼©æ”¾å› å­: {scale_factor}")
        scaled_bands = {}
        optical_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']

        for band_name, band_data in bands.items():
            if band_name in optical_bands:
                scaled_bands[band_name] = band_data * scale_factor
            else:
                scaled_bands[band_name] = band_data
        return scaled_bands


class OTSUThreshold:
    @staticmethod
    def calculate_otsu_threshold(data: np.ndarray, bins: int = 256) -> Tuple[float, Dict]:
        valid_data = data[~np.isnan(data)]
        if len(valid_data) == 0:
            raise ValueError("æ•°æ®å…¨ä¸ºNaN")

        hist, bin_edges = np.histogram(valid_data, bins=bins)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        hist = hist.astype(float)
        prob = hist / hist.sum()

        max_variance = 0
        optimal_threshold = 0

        w0 = np.cumsum(prob)
        w1 = 1 - w0
        mu = np.cumsum(prob * bin_centers)
        mu_total = mu[-1]

        for i in range(1, bins):
            if w0[i] == 0 or w1[i] == 0:
                continue

            mu0 = mu[i] / w0[i] if w0[i] > 0 else 0
            mu1 = (mu_total - mu[i]) / w1[i] if w1[i] > 0 else 0
            variance = w0[i] * w1[i] * (mu0 - mu1) ** 2

            if variance > max_variance:
                max_variance = variance
                optimal_threshold = bin_centers[i]

        threshold_idx = np.argmin(np.abs(bin_centers - optimal_threshold))

        metrics = {
            'threshold': optimal_threshold,
            'max_variance': max_variance,
            'histogram': hist,
            'bin_centers': bin_centers,
            'threshold_idx': threshold_idx,
            'background_prob': w0[threshold_idx],
            'foreground_prob': w1[threshold_idx]
        }

        return optimal_threshold, metrics


class WaterMaskGenerator:
    @staticmethod
    def calculate_mndwi(green: np.ndarray, swir1: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            mndwi = (green - swir1) / (green + swir1)
        return mndwi

    @staticmethod
    def calculate_ndwi(green: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndwi = (green - nir) / (green + nir)
        return ndwi

    @staticmethod
    def calculate_aweish(blue: np.ndarray, green: np.ndarray,
                         nir: np.ndarray, swir1: np.ndarray,
                         swir2: np.ndarray) -> np.ndarray:
        aweish = blue + 2.5 * green - 1.5 * (nir + swir1) - 0.25 * swir2
        return aweish

    @staticmethod
    def create_water_mask(bands: Dict[str, np.ndarray],
                          method: str = 'MNDWI',
                          threshold: Optional[float] = None,
                          use_otsu: bool = True) -> Tuple[np.ndarray, np.ndarray, float]:
        st.info(f"ğŸ’§ åˆ›å»ºæ°´ä½“æ©è†œ (æ–¹æ³•: {method})")

        if method.upper() == 'NDWI':
            water_index = WaterMaskGenerator.calculate_ndwi(bands['green'], bands['nir'])
        elif method.upper() == 'MNDWI':
            water_index = WaterMaskGenerator.calculate_mndwi(bands['green'], bands['swir1'])
        elif method.upper() == 'AWEISH':
            water_index = WaterMaskGenerator.calculate_aweish(
                bands['blue'], bands['green'], bands['nir'],
                bands['swir1'], bands['swir2']
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ°´ä½“æŒ‡æ•°: {method}")

        if use_otsu and threshold is None:
            try:
                otsu_threshold, metrics = OTSUThreshold.calculate_otsu_threshold(water_index, bins=256)
                final_threshold = otsu_threshold
                st.success(f"âœ“ OTSUé˜ˆå€¼: {final_threshold:.4f}")
            except Exception as e:
                st.warning(f"âš ï¸ OTSUå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é˜ˆå€¼0.0")
                final_threshold = 0.0
        elif threshold is not None:
            final_threshold = threshold
        else:
            final_threshold = 0.0

        water_mask = water_index > final_threshold

        total_pixels = np.sum(~np.isnan(water_index))
        water_pixels = np.sum(water_mask)
        water_ratio = water_pixels / total_pixels * 100 if total_pixels > 0 else 0

        st.write(f"æ°´åŸŸ: {water_pixels:,} ({water_ratio:.2f}%)")

        return water_index, water_mask, final_threshold


class RemoteSensingIndices:
    @staticmethod
    def calculate_ndvi(red: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndvi = (nir - red) / (nir + red)
        return np.clip(ndvi, -1, 1)

    @staticmethod
    def calculate_ndbi(swir1: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndbi = (swir1 - nir) / (swir1 + nir)
        return ndbi

    @staticmethod
    def calculate_ndbsi(bands: Dict[str, np.ndarray]) -> np.ndarray:
        swir1, red, nir, blue = bands['swir1'], bands['red'], bands['nir'], bands['blue']
        with np.errstate(divide='ignore', invalid='ignore'):
            si = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue))
            ndbi = RemoteSensingIndices.calculate_ndbi(swir1, nir)
            ndbsi = (si + ndbi) / 2
        return ndbsi

    @staticmethod
    def calculate_wet(bands: Dict[str, np.ndarray], sensor: str = 'Landsat8') -> np.ndarray:
        coeffs = {
            'blue': 0.1511, 'green': 0.1973, 'red': 0.3283,
            'nir': 0.3407, 'swir1': -0.7117, 'swir2': -0.4559
        }
        wet = np.zeros_like(bands['blue'])
        for band_name, coeff in coeffs.items():
            if band_name in bands:
                wet += coeff * bands[band_name]
        return wet

    @staticmethod
    def calculate_lst_simple(tir: np.ndarray) -> np.ndarray:
        if np.nanmax(tir) > 400:
            ML = 3.3420E-04
            AL = 0.10000
            K1 = 774.8853
            K2 = 1321.0789
            L_lambda = ML * tir + AL
            with np.errstate(divide='ignore', invalid='ignore'):
                T_b = K2 / np.log(K1 / L_lambda + 1)
        else:
            T_b = tir
        return T_b - 273.15


class RSEICalculator:
    def __init__(self, config: RSEIConfig = None):
        self.config = config or RSEIConfig()
        self.indices = {}
        self.rsei = None
        self.rsei_components = None
        self.calculated_breaks = None
        self.jenks_time = 0

    def normalize(self, array: np.ndarray, inverse: bool = False) -> np.ndarray:
        valid_mask = ~np.isnan(array)
        if not valid_mask.any():
            return array

        min_val = np.nanmin(array)
        max_val = np.nanmax(array)

        if max_val == min_val:
            return np.ones_like(array) * 0.5

        normalized = (array - min_val) / (max_val - min_val)
        if inverse:
            normalized = 1 - normalized
        return normalized

    def apply_water_mask(self, array: np.ndarray, water_mask: np.ndarray) -> np.ndarray:
        masked_array = array.copy()
        masked_array[water_mask] = np.nan
        return masked_array

    def calculate_rsei_pca(self, greenness: np.ndarray, wetness: np.ndarray,
                           dryness: np.ndarray, heat: np.ndarray,
                           water_mask: Optional[np.ndarray] = None) -> np.ndarray:
        st.info("ğŸ”¬ è®¡ç®—RSEI (PCAæ–¹æ³•)...")

        if water_mask is not None:
            greenness = self.apply_water_mask(greenness, water_mask)
            wetness = self.apply_water_mask(wetness, water_mask)
            dryness = self.apply_water_mask(dryness, water_mask)
            heat = self.apply_water_mask(heat, water_mask)

        green_norm = self.normalize(greenness, inverse=False)
        wet_norm = self.normalize(wetness, inverse=False)
        dry_norm = self.normalize(dryness, inverse=True)
        heat_norm = self.normalize(heat, inverse=True)

        mask = ~(np.isnan(green_norm) | np.isnan(wet_norm) |
                 np.isnan(dry_norm) | np.isnan(heat_norm))

        n_valid = mask.sum()
        st.write(f"æœ‰æ•ˆåƒç´ : {n_valid:,}")

        if n_valid < 100:
            raise ValueError("æœ‰æ•ˆåƒç´ å¤ªå°‘")

        data_matrix = np.column_stack([
            green_norm[mask], wet_norm[mask],
            dry_norm[mask], heat_norm[mask]
        ])

        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data_matrix)

        pca = PCA(n_components=4)
        pc_all = pca.fit_transform(data_scaled)
        pc1 = pc_all[:, 0]

        rsei_raw = np.full(greenness.shape, np.nan)
        rsei_raw[mask] = pc1.flatten()
        rsei = self.normalize(rsei_raw, inverse=False)

        st.success(f"PC1è´¡çŒ®ç‡: {pca.explained_variance_ratio_[0] * 100:.2f}%")

        self.rsei_components = {
            'greenness': green_norm,
            'wetness': wet_norm,
            'dryness': dry_norm,
            'heat': heat_norm,
            'pca': {
                'variance_ratio': pca.explained_variance_ratio_.tolist(),
                'components': pca.components_.tolist()
            }
        }

        if self.config.use_jenks:
            st.info("ğŸ” è®¡ç®—Jenksè‡ªç„¶é—´æ–­ç‚¹é˜ˆå€¼...")
            start_time = time.time()
            try:
                breaks = JenksNaturalBreaks.calculate_jenks_breaks(
                    rsei,
                    n_classes=5,
                    max_samples=self.config.jenks_samples
                )
                self.calculated_breaks = breaks
                self.jenks_time = time.time() - start_time
                st.success(f"âœ“ Jenksé˜ˆå€¼: {[f'{b:.4f}' for b in breaks]}")
                st.success(f"âœ“ è®¡ç®—è€—æ—¶: {self.jenks_time:.2f}ç§’")
            except Exception as e:
                st.warning(f"âš ï¸ Jenksè®¡ç®—å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é˜ˆå€¼")
                self.calculated_breaks = JenksNaturalBreaks.get_default_breaks()
                self.jenks_time = 0
        else:
            self.calculated_breaks = self.config.classification_breaks

        return rsei

    def classify_rsei(self, rsei: np.ndarray, breaks: Optional[List[float]] = None) -> np.ndarray:
        if breaks is None:
            breaks = self.calculated_breaks or self.config.classification_breaks

        classified = np.full_like(rsei, np.nan)

        if len(breaks) < 4:
            breaks = JenksNaturalBreaks.get_default_breaks()

        t1, t2, t3, t4 = breaks[0], breaks[1], breaks[2], breaks[3]

        classified[rsei < t1] = 1
        classified[(rsei >= t1) & (rsei < t2)] = 2
        classified[(rsei >= t2) & (rsei < t3)] = 3
        classified[(rsei >= t3) & (rsei < t4)] = 4
        classified[rsei >= t4] = 5

        return classified


class RSEIVisualizer:
    @staticmethod
    def create_comprehensive_visualization(rsei, rsei_class, indices,
                                           water_index=None, water_threshold=None,
                                           classification_breaks=None):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾"""

        rsei_colors = ['#d73027', '#fc8d59', '#fee08b', '#91cf60', '#1a9850']
        rsei_cmap = ListedColormap(rsei_colors)

        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)

        # 1. RSEIè¿ç»­å€¼
        ax1 = fig.add_subplot(gs[0, 0])
        im1 = ax1.imshow(rsei, cmap='RdYlGn', vmin=0, vmax=1)
        ax1.set_title('RSEI (è¿ç»­å€¼)', fontsize=12, fontweight='bold')
        ax1.axis('off')
        plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)

        # 2. RSEIåˆ†ç±»
        ax2 = fig.add_subplot(gs[0, 1])
        im2 = ax2.imshow(rsei_class, cmap=rsei_cmap, vmin=1, vmax=5)

        if classification_breaks:
            breaks_str = f"[{classification_breaks[0]:.2f}, {classification_breaks[1]:.2f}, " \
                         f"{classification_breaks[2]:.2f}, {classification_breaks[3]:.2f}]"
            ax2.set_title(f'RSEIåˆ†ç±»\né˜ˆå€¼: {breaks_str}', fontsize=10, fontweight='bold')
        else:
            ax2.set_title('RSEIåˆ†ç±»', fontsize=12, fontweight='bold')
        ax2.axis('off')

        class_names = ['å·®', 'è¾ƒå·®', 'ä¸­ç­‰', 'è‰¯å¥½', 'ä¼˜ç§€']
        legend_elements = [Patch(facecolor=rsei_colors[i], label=class_names[i])
                           for i in range(5)]
        ax2.legend(handles=legend_elements, loc='upper right', fontsize=8)

        # 3-6. å››ä¸ªæŒ‡æ•°
        indices_to_plot = [
            ('ndvi', 'NDVI (ç»¿åº¦)', 'Greens'),
            ('wet', 'WET (æ¹¿åº¦)', 'Blues'),
            ('ndbsi', 'NDBSI (å¹²åº¦)', 'YlOrRd'),
            ('lst', 'LST (çƒ­åº¦)', 'hot')
        ]

        for idx, (key, title, cmap) in enumerate(indices_to_plot):
            ax = fig.add_subplot(gs[0, 2 + idx % 2] if idx < 2 else gs[1, idx - 2])
            if key in indices:
                im = ax.imshow(indices[key], cmap=cmap)
                ax.set_title(title, fontsize=12, fontweight='bold')
                ax.axis('off')
                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

        # 7. æ°´ä½“æ©è†œ
        if water_index is not None:
            ax7 = fig.add_subplot(gs[1, 2])
            im7 = ax7.imshow(water_index, cmap='RdYlBu')
            ax7.set_title(f'æ°´ä½“æŒ‡æ•° (é˜ˆå€¼={water_threshold:.3f})', fontsize=12, fontweight='bold')
            ax7.axis('off')
            plt.colorbar(im7, ax=ax7, fraction=0.046, pad=0.04)

        # 8. RSEIç»Ÿè®¡ç›´æ–¹å›¾
        ax8 = fig.add_subplot(gs[1, 3])
        valid_rsei = rsei[~np.isnan(rsei)]
        ax8.hist(valid_rsei, bins=50, color='steelblue', edgecolor='black', alpha=0.7)
        ax8.axvline(np.nanmean(rsei), color='red', linestyle='--',
                    label=f'å‡å€¼={np.nanmean(rsei):.3f}', linewidth=2)

        if classification_breaks:
            colors_line = ['#d73027', '#fc8d59', '#fee08b', '#91cf60']
            for i, threshold in enumerate(classification_breaks):
                ax8.axvline(threshold, color=colors_line[i], linestyle=':',
                            linewidth=1.5, alpha=0.7)

        ax8.set_title('RSEIåˆ†å¸ƒ', fontsize=12, fontweight='bold')
        ax8.set_xlabel('RSEIå€¼')
        ax8.set_ylabel('é¢‘æ•°')
        ax8.legend(fontsize=8)
        ax8.grid(alpha=0.3)

        # 9. ç­‰çº§é¢ç§¯ç»Ÿè®¡
        ax9 = fig.add_subplot(gs[2, :2])
        class_counts = [np.sum(rsei_class == i) for i in range(1, 6)]
        colors = rsei_colors
        bars = ax9.bar(class_names, class_counts, color=colors, edgecolor='black', alpha=0.8)
        ax9.set_title('RSEIç­‰çº§é¢ç§¯ç»Ÿè®¡', fontsize=12, fontweight='bold')
        ax9.set_ylabel('åƒç´ æ•°é‡')
        ax9.grid(axis='y', alpha=0.3)

        for bar, count in zip(bars, class_counts):
            height = bar.get_height()
            ax9.text(bar.get_x() + bar.get_width() / 2., height,
                     f'{int(count):,}\n({count / np.sum(class_counts) * 100:.1f}%)',
                     ha='center', va='bottom', fontsize=9)

        # 10. ç­‰çº§é¢ç§¯é¥¼å›¾
        ax10 = fig.add_subplot(gs[2, 2:])
        wedges, texts, autotexts = ax10.pie(class_counts, labels=class_names,
                                            colors=colors, autopct='%1.1f%%',
                                            startangle=90)
        ax10.set_title('RSEIç­‰çº§æ¯”ä¾‹', fontsize=12, fontweight='bold')

        for text in texts:
            text.set_fontsize(10)
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')

        title_text = 'RSEIç»¼åˆåˆ†æç»“æœ'
        if classification_breaks:
            method = "Jenksè‡ªç„¶é—´æ–­ç‚¹" if classification_breaks != [0.2, 0.4, 0.6, 0.8] else "ç­‰é—´è·"
            title_text += f' ({method}åˆ†ç±»)'
        fig.suptitle(title_text, fontsize=16, fontweight='bold', y=0.98)

        return fig


# =============================
# æ ¸å¿ƒè®¡ç®—å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
# =============================
def execute_rsei_calculation(input_file, config):
    """æ ¸å¿ƒè®¡ç®—é€»è¾‘ - å¢å¼ºç‰ˆï¼Œå¯¼å‡ºæ‰€æœ‰æŒ‡æ•°"""
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    output_path = Path(temp_dir)

    try:
        progress_bar = st.progress(0)
        status_text = st.empty()

        # 1. è¯»å–å½±åƒ
        status_text.text("æ­¥éª¤1/9: è¯»å–å½±åƒ...")
        progress_bar.progress(10)
        reader = MultiSpectralImageReader(config)
        bands = reader.read_multiband_tif(input_file)

        # 2. é¢„å¤„ç†
        status_text.text("æ­¥éª¤2/9: æ•°æ®é¢„å¤„ç†...")
        progress_bar.progress(15)
        max_val = np.nanmax(bands['red'])
        if max_val > 1.0:
            bands = reader.apply_scale_factor(bands, 0.0001)

        # 3. æ°´ä½“æ©è†œ
        water_index = None
        water_mask = None
        water_threshold_used = None

        if config.mask_water:
            status_text.text("æ­¥éª¤3/9: åˆ›å»ºæ°´ä½“æ©è†œ...")
            progress_bar.progress(25)
            water_index, water_mask, water_threshold_used = WaterMaskGenerator.create_water_mask(
                bands, config.water_index, config.water_threshold, config.use_otsu
            )

        # 4. è®¡ç®—æŒ‡æ•°
        status_text.text("æ­¥éª¤4/9: è®¡ç®—é¥æ„ŸæŒ‡æ•°...")
        progress_bar.progress(35)
        calc = RemoteSensingIndices()
        ndvi = calc.calculate_ndvi(bands['red'], bands['nir'])
        wet = calc.calculate_wet(bands, config.satellite)
        ndbsi = calc.calculate_ndbsi(bands)

        # é¢å¤–è®¡ç®—NDBIå’ŒSIï¼ˆç”¨äºå®Œæ•´å¯¼å‡ºï¼‰
        ndbi = calc.calculate_ndbi(bands['swir1'], bands['nir'])
        swir1, red, nir, blue = bands['swir1'], bands['red'], bands['nir'], bands['blue']
        with np.errstate(divide='ignore', invalid='ignore'):
            si = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue))

        if 'tir' in bands and bands['tir'] is not None:
            lst = calc.calculate_lst_simple(bands['tir'])
        else:
            lst = ndbsi

        # å®Œæ•´çš„æŒ‡æ•°å­—å…¸
        indices = {
            'ndvi': ndvi,
            'wet': wet,
            'ndbsi': ndbsi,
            'lst': lst,
            'ndbi': ndbi,
            'si': si
        }

        # 5. è®¡ç®—RSEI
        status_text.text("æ­¥éª¤5/9: è®¡ç®—RSEI...")
        progress_bar.progress(45)
        rsei_calc = RSEICalculator(config)
        rsei = rsei_calc.calculate_rsei_pca(ndvi, wet, ndbsi, lst, water_mask)

        # 6. åˆ†ç±»
        status_text.text("æ­¥éª¤6/9: RSEIåˆ†ç±»...")
        progress_bar.progress(55)
        classification_breaks = rsei_calc.calculated_breaks
        rsei_class = rsei_calc.classify_rsei(rsei, classification_breaks)

        # ç»Ÿè®¡
        class_names = ['å·®', 'è¾ƒå·®', 'ä¸­ç­‰', 'è‰¯å¥½', 'ä¼˜ç§€']
        total_valid = np.sum(~np.isnan(rsei_class))

        st.write(f"\nä½¿ç”¨çš„åˆ†ç±»é˜ˆå€¼: {[f'{b:.4f}' for b in classification_breaks]}")
        st.write("\nç­‰çº§åˆ†å¸ƒ:")
        for i, name in enumerate(class_names, 1):
            count = np.sum(rsei_class == i)
            ratio = count / total_valid * 100 if total_valid > 0 else 0
            st.write(f"{name}: {count:,} ({ratio:.2f}%)")

        # 7. ç”Ÿæˆå¯è§†åŒ–
        status_text.text("æ­¥éª¤7/9: ç”Ÿæˆå¯è§†åŒ–å›¾...")
        progress_bar.progress(65)
        fig = RSEIVisualizer.create_comprehensive_visualization(
            rsei, rsei_class, indices, water_index,
            water_threshold_used, classification_breaks
        )

        # ä¿å­˜å›¾ç‰‡
        img_path = output_path / 'RSEI_comprehensive.png'
        fig.savefig(img_path, dpi=300, bbox_inches='tight')
        plt.close(fig)

        # 8. å¯¼å‡ºGeoTIFFæ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰æŒ‡æ•°ï¼‰
        status_text.text("æ­¥éª¤8/9: å¯¼å‡ºGeoTIFFæ–‡ä»¶...")
        progress_bar.progress(75)

        saved_files = []

        if config.export_geotiff and reader.metadata:
            # ä¸»è¦ç»“æœ
            with rasterio.open(output_path / 'RSEI.tif', 'w', **reader.metadata) as dst:
                dst.write(rsei.astype('float32'), 1)
            saved_files.append('RSEI.tif')

            with rasterio.open(output_path / 'RSEI_classified.tif', 'w', **reader.metadata) as dst:
                dst.write(rsei_class.astype('float32'), 1)
            saved_files.append('RSEI_classified.tif')

            # æ°´ä½“ç›¸å…³
            if water_index is not None:
                with rasterio.open(output_path / 'Water_Index.tif', 'w', **reader.metadata) as dst:
                    dst.write(water_index.astype('float32'), 1)
                saved_files.append('Water_Index.tif')

                with rasterio.open(output_path / 'Water_Mask.tif', 'w', **reader.metadata) as dst:
                    dst.write(water_mask.astype('float32'), 1)
                saved_files.append('Water_Mask.tif')

            # å¯¼å‡ºæ‰€æœ‰é¥æ„ŸæŒ‡æ•°
            if config.export_indices:
                st.info("æ­£åœ¨å¯¼å‡ºæ‰€æœ‰é¥æ„ŸæŒ‡æ•°...")

                # NDVI
                with rasterio.open(output_path / 'NDVI.tif', 'w', **reader.metadata) as dst:
                    dst.write(ndvi.astype('float32'), 1)
                saved_files.append('NDVI.tif')

                # WET
                with rasterio.open(output_path / 'WET.tif', 'w', **reader.metadata) as dst:
                    dst.write(wet.astype('float32'), 1)
                saved_files.append('WET.tif')

                # NDBSI
                with rasterio.open(output_path / 'NDBSI.tif', 'w', **reader.metadata) as dst:
                    dst.write(ndbsi.astype('float32'), 1)
                saved_files.append('NDBSI.tif')

                # LST
                with rasterio.open(output_path / 'LST.tif', 'w', **reader.metadata) as dst:
                    dst.write(lst.astype('float32'), 1)
                saved_files.append('LST.tif')

                # NDBI
                with rasterio.open(output_path / 'NDBI.tif', 'w', **reader.metadata) as dst:
                    dst.write(ndbi.astype('float32'), 1)
                saved_files.append('NDBI.tif')

                # SI
                with rasterio.open(output_path / 'SI.tif', 'w', **reader.metadata) as dst:
                    dst.write(si.astype('float32'), 1)
                saved_files.append('SI.tif')

                # å½’ä¸€åŒ–åçš„å››ä¸ªæŒ‡æ ‡
                greenness = rsei_calc.rsei_components['greenness']
                wetness_norm = rsei_calc.rsei_components['wetness']
                dryness = rsei_calc.rsei_components['dryness']
                heat = rsei_calc.rsei_components['heat']

                with rasterio.open(output_path / 'Greenness_Normalized.tif', 'w', **reader.metadata) as dst:
                    dst.write(greenness.astype('float32'), 1)
                saved_files.append('Greenness_Normalized.tif')

                with rasterio.open(output_path / 'Wetness_Normalized.tif', 'w', **reader.metadata) as dst:
                    dst.write(wetness_norm.astype('float32'), 1)
                saved_files.append('Wetness_Normalized.tif')

                with rasterio.open(output_path / 'Dryness_Normalized.tif', 'w', **reader.metadata) as dst:
                    dst.write(dryness.astype('float32'), 1)
                saved_files.append('Dryness_Normalized.tif')

                with rasterio.open(output_path / 'Heat_Normalized.tif', 'w', **reader.metadata) as dst:
                    dst.write(heat.astype('float32'), 1)
                saved_files.append('Heat_Normalized.tif')

        # 9. Excelç»Ÿè®¡
        status_text.text("æ­¥éª¤9/9: ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        progress_bar.progress(85)

        # æ‰©å±•ç»Ÿè®¡è¡¨ï¼ŒåŒ…å«æ‰€æœ‰æŒ‡æ•°
        stats_df = pd.DataFrame({
            'æŒ‡æ ‡': ['NDVI', 'WET', 'NDBSI', 'LST', 'NDBI', 'SI', 'RSEI'],
            'æœ€å°å€¼': [f"{np.nanmin(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'æœ€å¤§å€¼': [f"{np.nanmax(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'å‡å€¼': [f"{np.nanmean(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'æ ‡å‡†å·®': [f"{np.nanstd(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]]
        })

        class_df = pd.DataFrame({
            'ç­‰çº§': class_names,
            'åƒç´ æ•°': [int(np.sum(rsei_class == i)) for i in range(1, 6)],
            'ç™¾åˆ†æ¯”': [f"{np.sum(rsei_class == i) / total_valid * 100:.2f}%" for i in range(1, 6)]
        })

        threshold_df = pd.DataFrame({
            'åˆ†ç±»é˜ˆå€¼': ['å·®/è¾ƒå·®', 'è¾ƒå·®/ä¸­ç­‰', 'ä¸­ç­‰/è‰¯å¥½', 'è‰¯å¥½/ä¼˜ç§€'],
            'é˜ˆå€¼': [f"{b:.4f}" for b in classification_breaks],
            'æ–¹æ³•': ['Jenksè‡ªç„¶é—´æ–­ç‚¹' if config.use_jenks else 'æ‰‹åŠ¨è®¾ç½®'] * 4,
            'Jenksè€—æ—¶(ç§’)': [f"{rsei_calc.jenks_time:.2f}" if config.use_jenks else 'N/A'] * 4
        })

        # æ–‡ä»¶æ¸…å•
        files_df = pd.DataFrame({
            'æ–‡ä»¶å': saved_files,
            'è¯´æ˜': [
                'RSEIè¿ç»­å€¼ï¼ˆ0-1ï¼‰',
                'RSEIåˆ†ç±»ï¼ˆ1-5ï¼‰',
                'æ°´ä½“æŒ‡æ•°',
                'æ°´ä½“æ©è†œ',
                'å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•°',
                'æ¹¿åº¦æŒ‡æ•°',
                'å½’ä¸€åŒ–å»ºç­‘-åœŸå£¤æŒ‡æ•°',
                'åœ°è¡¨æ¸©åº¦',
                'å½’ä¸€åŒ–å»ºç­‘æŒ‡æ•°',
                'åœŸå£¤æŒ‡æ•°',
                'ç»¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'æ¹¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'å¹²åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'çƒ­åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰'
            ][:len(saved_files)]
        })

        excel_path = output_path / 'RSEI_analysis.xlsx'
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            stats_df.to_excel(writer, sheet_name='æŒ‡æ ‡ç»Ÿè®¡', index=False)
            class_df.to_excel(writer, sheet_name='ç­‰çº§åˆ†å¸ƒ', index=False)
            threshold_df.to_excel(writer, sheet_name='åˆ†ç±»é˜ˆå€¼', index=False)
            files_df.to_excel(writer, sheet_name='æ–‡ä»¶æ¸…å•', index=False)

        progress_bar.progress(100)
        status_text.text("âœ… è®¡ç®—å®Œæˆï¼")

        return {
            'rsei': rsei,
            'rsei_class': rsei_class,
            'indices': indices,
            'stats_df': stats_df,
            'class_df': class_df,
            'threshold_df': threshold_df,
            'files_df': files_df,
            'img_path': str(img_path),
            'excel_path': str(excel_path),
            'output_path': output_path,
            'classification_breaks': classification_breaks,
            'saved_files': saved_files
        }

    except Exception as e:
        st.error(f"è®¡ç®—å¤±è´¥: {e}")
        import traceback
        st.error(traceback.format_exc())
        return None


# =============================
# Streamlitä¸»ç¨‹åº
# =============================
def main():
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸŒ¿ RSEIè®¡ç®—ç³»ç»Ÿ v3.6 - å®Œæ•´å¯¼å‡ºç‰ˆ")
    st.markdown("**Remote Sensing based Ecological Index é¥æ„Ÿç”Ÿæ€æŒ‡æ•°è®¡ç®—å·¥å…·**")

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ å‚æ•°é…ç½®")

        # æ–‡ä»¶ä¸Šä¼ 
        st.subheader("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
        uploaded_file = st.file_uploader(
            "é€‰æ‹©å¤šæ³¢æ®µTIFå½±åƒ",
            type=['tif', 'tiff'],
            help="ä¸Šä¼ Landsat 8æˆ–Sentinel-2çš„å¤šæ³¢æ®µå½±åƒæ–‡ä»¶"
        )

        # å«æ˜Ÿå‚æ•°
        st.subheader("ğŸ›°ï¸ å«æ˜Ÿå‚æ•°")
        satellite = st.selectbox(
            "å«æ˜Ÿç±»å‹",
            ["Landsat8", "Sentinel2"],
            index=0
        )

        # è®¡ç®—æ–¹æ³•
        st.subheader("ğŸ”¬ è®¡ç®—æ–¹æ³•")
        use_pca = st.checkbox("ä½¿ç”¨PCAæ–¹æ³•", value=True, help="æ¨èä½¿ç”¨PCAæ–¹æ³•è®¡ç®—RSEI")

        # åˆ†ç±»é˜ˆå€¼è®¾ç½®
        st.subheader("ğŸ“Š åˆ†ç±»é˜ˆå€¼è®¾ç½®")
        use_jenks = st.checkbox(
            "ä½¿ç”¨Jenksè‡ªç„¶é—´æ–­ç‚¹",
            value=True,
            help="è‡ªåŠ¨è®¡ç®—æœ€ä¼˜åˆ†ç±»é˜ˆå€¼"
        )

        if use_jenks:
            jenks_samples = st.slider(
                "é‡‡æ ·æ•°é‡",
                min_value=1000,
                max_value=20000,
                value=5000,
                step=1000,
                help="è¶Šå¤§è¶Šç²¾ç¡®ä½†è¶Šæ…¢"
            )
        else:
            st.write("æ‰‹åŠ¨è®¾ç½®é˜ˆå€¼:")
            threshold_1 = st.number_input("å·®/è¾ƒå·®", min_value=0.0, max_value=1.0, value=0.2, step=0.01)
            threshold_2 = st.number_input("è¾ƒå·®/ä¸­ç­‰", min_value=0.0, max_value=1.0, value=0.4, step=0.01)
            threshold_3 = st.number_input("ä¸­ç­‰/è‰¯å¥½", min_value=0.0, max_value=1.0, value=0.6, step=0.01)
            threshold_4 = st.number_input("è‰¯å¥½/ä¼˜ç§€", min_value=0.0, max_value=1.0, value=0.8, step=0.01)

            col1, col2 = st.columns(2)
            with col1:
                if st.button("ç­‰é—´è·"):
                    threshold_1, threshold_2, threshold_3, threshold_4 = 0.2, 0.4, 0.6, 0.8
            with col2:
                if st.button("å››åˆ†ä½æ•°"):
                    threshold_1, threshold_2, threshold_3, threshold_4 = 0.25, 0.50, 0.75, 0.90

        # æ°´ä½“æ©è†œ
        st.subheader("ğŸ’§ æ°´ä½“æ©è†œ")
        mask_water = st.checkbox("å»é™¤æ°´åŸŸ", value=True)

        if mask_water:
            water_index = st.selectbox(
                "æ°´ä½“æŒ‡æ•°",
                ["MNDWI", "NDWI", "AWEIsh"],
                index=0
            )

            use_otsu = st.checkbox("ä½¿ç”¨OTSUè‡ªåŠ¨è®¡ç®—é˜ˆå€¼", value=True)

            if not use_otsu:
                water_threshold = st.number_input(
                    "æ‰‹åŠ¨é˜ˆå€¼",
                    min_value=-1.0,
                    max_value=1.0,
                    value=0.0,
                    step=0.1
                )
            else:
                water_threshold = None
        else:
            water_index = "MNDWI"
            use_otsu = True
            water_threshold = None

        # å¯¼å‡ºé€‰é¡¹
        st.subheader("ğŸ’¾ å¯¼å‡ºé€‰é¡¹")
        export_geotiff = st.checkbox("å¯¼å‡ºGeoTIFFæ–‡ä»¶", value=True)
        export_indices = st.checkbox(
            "å¯¼å‡ºæ‰€æœ‰é¥æ„ŸæŒ‡æ•°",
            value=True,
            help="å¯¼å‡ºNDVI, WET, NDBSI, LST, NDBI, SIåŠå½’ä¸€åŒ–æŒ‡æ ‡"
        )

        st.markdown("---")
        st.markdown("""
        **å¯¼å‡ºå†…å®¹:**

        âœ… **ä¸»è¦ç»“æœ**
        - RSEI.tif (è¿ç»­å€¼)
        - RSEI_classified.tif (åˆ†ç±»)
        - ç»¼åˆå¯è§†åŒ–å›¾
        - Excelç»Ÿè®¡æŠ¥å‘Š

        âœ… **é¥æ„ŸæŒ‡æ•°** (å¯é€‰)
        - NDVI (æ¤è¢«æŒ‡æ•°)
        - WET (æ¹¿åº¦æŒ‡æ•°)
        - NDBSI (å»ºç­‘-åœŸå£¤æŒ‡æ•°)
        - LST (åœ°è¡¨æ¸©åº¦)
        - NDBI (å»ºç­‘æŒ‡æ•°)
        - SI (åœŸå£¤æŒ‡æ•°)
        - å½’ä¸€åŒ–å››æŒ‡æ ‡

        âœ… **æ°´ä½“åˆ†æ** (å¯é€‰)
        - æ°´ä½“æŒ‡æ•°
        - æ°´ä½“æ©è†œ
        """)

    # ä¸»å†…å®¹åŒº
    if uploaded_file is not None:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
        with tempfile.NamedTemporaryFile(delete=False, suffix='.tif') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.success(f"âœ… æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")

        file_size = len(uploaded_file.getvalue()) / (1024 * 1024)  # MB
        st.info(f"ğŸ“¦ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

        # å¼€å§‹è®¡ç®—æŒ‰é’®
        if st.button("â–¶ï¸ å¼€å§‹è®¡ç®—", type="primary"):
            # éªŒè¯é˜ˆå€¼
            if not use_jenks:
                thresholds = [threshold_1, threshold_2, threshold_3, threshold_4]
                if not all(thresholds[i] < thresholds[i + 1] for i in range(3)):
                    st.error("âŒ é˜ˆå€¼å¿…é¡»é€’å¢ï¼è¯·ç¡®ä¿: é˜ˆå€¼1 < é˜ˆå€¼2 < é˜ˆå€¼3 < é˜ˆå€¼4")
                    return

            # åˆ›å»ºé…ç½®
            config = RSEIConfig(
                satellite=satellite,
                use_pca=use_pca,
                export_indices=export_indices,
                export_geotiff=export_geotiff,
                mask_water=mask_water,
                water_index=water_index,
                water_threshold=water_threshold,
                use_otsu=use_otsu,
                use_jenks=use_jenks,
                classification_breaks=[threshold_1, threshold_2, threshold_3,
                                       threshold_4] if not use_jenks else [0.2, 0.4, 0.6, 0.8],
                jenks_samples=jenks_samples if use_jenks else 5000
            )

            # å¼€å§‹è®¡ç®—
            start_time = time.time()

            with st.spinner("è®¡ç®—ä¸­ï¼Œè¯·ç¨å€™..."):
                results = execute_rsei_calculation(tmp_file_path, config)

            elapsed_time = time.time() - start_time

            if results:
                st.success(f"âœ… è®¡ç®—å®Œæˆï¼è€—æ—¶: {elapsed_time:.1f}ç§’")

                # æ˜¾ç¤ºç»“æœ
                st.header("ğŸ“Š è®¡ç®—ç»“æœ")

                # åˆ›å»ºæ ‡ç­¾é¡µ
                tab1, tab2, tab3, tab4, tab5 = st.tabs([
                    "ğŸ“ˆ ç»Ÿè®¡æ•°æ®",
                    "ğŸ–¼ï¸ å¯è§†åŒ–ç»“æœ",
                    "ğŸ“¥ ä¸‹è½½æ–‡ä»¶",
                    "ğŸ“‹ æ–‡ä»¶æ¸…å•",
                    "â„¹ï¸ è¯¦ç»†ä¿¡æ¯"
                ])

                with tab1:
                    st.subheader("æŒ‡æ ‡ç»Ÿè®¡")
                    st.dataframe(results['stats_df'], use_container_width=True)

                    st.subheader("ç­‰çº§åˆ†å¸ƒ")
                    st.dataframe(results['class_df'], use_container_width=True)

                    st.subheader("åˆ†ç±»é˜ˆå€¼")
                    st.dataframe(results['threshold_df'], use_container_width=True)

                with tab2:
                    st.subheader("RSEIç»¼åˆåˆ†æå¯è§†åŒ–")
                    st.image(results['img_path'], use_column_width=True)

                with tab3:
                    st.subheader("ä¸‹è½½ç»“æœæ–‡ä»¶")

                    col1, col2 = st.columns(2)

                    with col1:
                        # ä¸‹è½½å¯è§†åŒ–å›¾
                        with open(results['img_path'], 'rb') as f:
                            st.download_button(
                                label="ğŸ“· ä¸‹è½½å¯è§†åŒ–å›¾ (PNG)",
                                data=f,
                                file_name="RSEI_comprehensive.png",
                                mime="image/png",
                                use_container_width=True
                            )

                    with col2:
                        # ä¸‹è½½Excel
                        with open(results['excel_path'], 'rb') as f:
                            st.download_button(
                                label="ğŸ“Š ä¸‹è½½ç»Ÿè®¡æŠ¥å‘Š (Excel)",
                                data=f,
                                file_name="RSEI_analysis.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                use_container_width=True
                            )

                    st.markdown("---")

                    # æ‰“åŒ…æ‰€æœ‰æ–‡ä»¶ä¸‹è½½
                    if export_geotiff:
                        st.subheader("ğŸ“¦ æ‰“åŒ…ä¸‹è½½")

                        # æ˜¾ç¤ºå°†è¦æ‰“åŒ…çš„æ–‡ä»¶
                        st.info(f"å°†æ‰“åŒ… {len(results['saved_files'])} ä¸ªæ–‡ä»¶")

                        with st.expander("æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨"):
                            for file in results['saved_files']:
                                st.text(f"âœ“ {file}")

                        # åˆ›å»ºZIP
                        zip_buffer = io.BytesIO()
                        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                            output_path = results['output_path']

                            # æ·»åŠ æ‰€æœ‰æ–‡ä»¶åˆ°ZIP
                            for file in output_path.glob('*'):
                                zip_file.write(file, file.name)

                            # åˆ›å»ºREADME
                            readme_content = f"""
RSEIè®¡ç®—ç»“æœè¯´æ˜
=================

è®¡ç®—æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
å«æ˜Ÿç±»å‹: {config.satellite}
åˆ†ç±»æ–¹æ³•: {'Jenksè‡ªç„¶é—´æ–­ç‚¹' if config.use_jenks else 'æ‰‹åŠ¨è®¾ç½®'}

æ–‡ä»¶è¯´æ˜:
---------
1. RSEI.tif - RSEIè¿ç»­å€¼ (0-1)
2. RSEI_classified.tif - RSEIåˆ†ç±» (1-5)
3. RSEI_comprehensive.png - ç»¼åˆå¯è§†åŒ–å›¾
4. RSEI_analysis.xlsx - ç»Ÿè®¡æŠ¥å‘Š

é¥æ„ŸæŒ‡æ•°:
---------
5. NDVI.tif - å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•°
6. WET.tif - æ¹¿åº¦æŒ‡æ•°
7. NDBSI.tif - å½’ä¸€åŒ–å»ºç­‘-åœŸå£¤æŒ‡æ•°
8. LST.tif - åœ°è¡¨æ¸©åº¦
9. NDBI.tif - å½’ä¸€åŒ–å»ºç­‘æŒ‡æ•°
10. SI.tif - åœŸå£¤æŒ‡æ•°

å½’ä¸€åŒ–æŒ‡æ ‡:
-----------
11. Greenness_Normalized.tif - ç»¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰
12. Wetness_Normalized.tif - æ¹¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰
13. Dryness_Normalized.tif - å¹²åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰
14. Heat_Normalized.tif - çƒ­åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰

æ°´ä½“åˆ†æ:
---------
15. Water_Index.tif - æ°´ä½“æŒ‡æ•°
16. Water_Mask.tif - æ°´ä½“æ©è†œ

åˆ†ç±»ç­‰çº§:
---------
1 - å·®
2 - è¾ƒå·®
3 - ä¸­ç­‰
4 - è‰¯å¥½
5 - ä¼˜ç§€

æ³¨æ„äº‹é¡¹:
---------
- æ‰€æœ‰GeoTIFFæ–‡ä»¶ä¿ç•™äº†åŸå§‹å½±åƒçš„åœ°ç†å‚è€ƒä¿¡æ¯
- å¯åœ¨ArcGISã€QGISç­‰GISè½¯ä»¶ä¸­æ‰“å¼€
- Excelæ–‡ä»¶åŒ…å«è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯å’Œæ–‡ä»¶æ¸…å•

å‚è€ƒæ–‡çŒ®:
---------
Xu, H., et al. (2013). A remote sensing urban ecological index and its application.
Acta Ecologica Sinica, 33(24), 7853-7862.
"""
                            zip_file.writestr('README.txt', readme_content.encode('utf-8'))

                        # è®¡ç®—ZIPå¤§å°
                        zip_size = len(zip_buffer.getvalue()) / (1024 * 1024)  # MB

                        st.download_button(
                            label=f"ğŸ“¦ ä¸‹è½½æ‰€æœ‰ç»“æœ (ZIP) - {zip_size:.2f} MB",
                            data=zip_buffer.getvalue(),
                            file_name=f"RSEI_results_{time.strftime('%Y%m%d_%H%M%S')}.zip",
                            mime="application/zip",
                            use_container_width=True
                        )

                        st.success("âœ… ZIPåŒ…åŒ…å«æ‰€æœ‰è®¡ç®—ç»“æœå’Œé¥æ„ŸæŒ‡æ•°ï¼")

                with tab4:
                    st.subheader("è¾“å‡ºæ–‡ä»¶æ¸…å•")
                    st.dataframe(results['files_df'], use_container_width=True)

                    st.info(f"å…±ç”Ÿæˆ {len(results['saved_files'])} ä¸ªæ–‡ä»¶")

                with tab5:
                    st.subheader("è®¡ç®—è¯¦æƒ…")

                    col1, col2 = st.columns(2)

                    with col1:
                        st.metric("å«æ˜Ÿç±»å‹", config.satellite)
                        st.metric("è®¡ç®—æ–¹æ³•", 'PCA' if config.use_pca else 'ç›´æ¥è®¡ç®—')
                        st.metric("åˆ†ç±»æ–¹æ³•", 'Jenksè‡ªç„¶é—´æ–­ç‚¹' if config.use_jenks else 'æ‰‹åŠ¨è®¾ç½®')
                        st.metric("æ€»è€—æ—¶", f"{elapsed_time:.1f}ç§’")

                    with col2:
                        st.metric("æ°´ä½“æ©è†œ", 'æ˜¯' if config.mask_water else 'å¦')
                        if config.mask_water:
                            st.metric("æ°´ä½“æŒ‡æ•°", config.water_index)
                            st.metric("é˜ˆå€¼æ–¹æ³•", 'OTSUè‡ªåŠ¨' if config.use_otsu else 'æ‰‹åŠ¨')
                        st.metric("å¯¼å‡ºæ–‡ä»¶æ•°", len(results['saved_files']))

                    st.markdown("---")
                    st.write("**åˆ†ç±»é˜ˆå€¼:**", [f'{b:.4f}' for b in results['classification_breaks']])

                    if config.use_jenks:
                        st.write(f"**Jenksè®¡ç®—è€—æ—¶:** {results['threshold_df']['Jenksè€—æ—¶(ç§’)'].iloc[0]}")

    else:
        # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ å¤šæ³¢æ®µTIFå½±åƒæ–‡ä»¶å¼€å§‹è®¡ç®—")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            ### ğŸŒŸ åŠŸèƒ½ç‰¹ç‚¹

            - âœ… æ”¯æŒ Landsat 8 å’Œ Sentinel-2 å½±åƒ
            - âœ… è‡ªåŠ¨æ°´ä½“æ©è†œï¼ˆOTSUé˜ˆå€¼ï¼‰
            - âœ… Jenksè‡ªç„¶é—´æ–­ç‚¹åˆ†ç±»
            - âœ… å¤šç§åŠ é€Ÿç®—æ³•ï¼ˆjenkspy/numba/numpyï¼‰
            - âœ… **å®Œæ•´å¯¼å‡ºæ‰€æœ‰é¥æ„ŸæŒ‡æ•°**
            - âœ… **ä¸€é”®æ‰“åŒ…ä¸‹è½½ZIP**
            - âœ… å®Œæ•´çš„å¯è§†åŒ–åˆ†æ

            ### ğŸ“‹ è¾“å…¥è¦æ±‚

            - **æ–‡ä»¶æ ¼å¼:** GeoTIFF (.tif/.tiff)
            - **æ³¢æ®µé¡ºåº:**
              - Landsat 8: è“ã€ç»¿ã€çº¢ã€è¿‘çº¢å¤–ã€çŸ­æ³¢çº¢å¤–1ã€çŸ­æ³¢çº¢å¤–2ã€çƒ­çº¢å¤–
              - Sentinel-2: è“ã€ç»¿ã€çº¢ã€è¿‘çº¢å¤–ã€çŸ­æ³¢çº¢å¤–1ã€çŸ­æ³¢çº¢å¤–2

            ### âš¡ æ€§èƒ½ä¼˜åŒ–

            å®‰è£…åŠ é€Ÿåº“å¯æå‡è®¡ç®—é€Ÿåº¦10-50å€:

            ```bash
            pip install jenkspy numba
            ```
            """)

        with col2:
            st.markdown("""
            ### ğŸ“Š è¾“å‡ºç»“æœï¼ˆå®Œæ•´ç‰ˆï¼‰

            **ä¸»è¦ç»“æœ:**
            - ğŸ¯ RSEIè¿ç»­å€¼å½±åƒ
            - ğŸ“Š RSEIåˆ†ç±»å½±åƒ (5çº§)
            - ğŸ–¼ï¸ ç»¼åˆåˆ†æå¯è§†åŒ–å›¾
            - ğŸ“ˆ Excelç»Ÿè®¡æŠ¥å‘Š

            **é¥æ„ŸæŒ‡æ•° (10+):**
            - ğŸŒ± NDVI (å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•°)
            - ğŸ’§ WET (æ¹¿åº¦æŒ‡æ•°)
            - ğŸ—ï¸ NDBSI (å»ºç­‘-åœŸå£¤æŒ‡æ•°)
            - ğŸŒ¡ï¸ LST (åœ°è¡¨æ¸©åº¦)
            - ğŸ¢ NDBI (å»ºç­‘æŒ‡æ•°)
            - ğŸŒ¾ SI (åœŸå£¤æŒ‡æ•°)
            - ğŸ”„ å½’ä¸€åŒ–å››æŒ‡æ ‡ (ç»¿åº¦/æ¹¿åº¦/å¹²åº¦/çƒ­åº¦)

            **æ°´ä½“åˆ†æ:**
            - ğŸŒŠ æ°´ä½“æŒ‡æ•° (MNDWI/NDWI/AWEIsh)
            - ğŸ—ºï¸ æ°´ä½“æ©è†œ

            ### ğŸ“š å‚è€ƒæ–‡çŒ®

            Xu, H., et al. (2013). A remote sensing urban ecological index and its application.
            *Acta Ecologica Sinica*, 33(24), 7853-7862.
            """)


if __name__ == "__main__":
    main()