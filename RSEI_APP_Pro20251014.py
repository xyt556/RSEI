import streamlit as st
from pathlib import Path
import numpy as np
import rasterio
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from matplotlib.patches import Patch
from matplotlib import font_manager
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd
import warnings
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass, field
import time
import io
import zipfile
import tempfile
import shutil
import platform

warnings.filterwarnings('ignore')


# =============================
# ä¸­æ–‡å­—ä½“é…ç½®ï¼ˆå½»åº•è§£å†³æ–¹æ¡ˆï¼‰
# =============================
def setup_chinese_font():
    """
    è‡ªåŠ¨æ£€æµ‹å¹¶è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œæä¾›å¤šé‡å¤‡é€‰æ–¹æ¡ˆ
    """
    # è·å–ç³»ç»Ÿç±»å‹
    system = platform.system()

    # æ ¹æ®ä¸åŒæ“ä½œç³»ç»Ÿè®¾ç½®å­—ä½“ä¼˜å…ˆçº§
    if system == 'Windows':
        font_list = ['Microsoft YaHei', 'SimHei', 'SimSun', 'KaiTi', 'FangSong']
    elif system == 'Darwin':  # macOS
        font_list = ['PingFang SC', 'Heiti SC', 'STHeiti', 'Arial Unicode MS']
    else:  # Linux
        font_list = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 'Noto Sans CJK SC',
                     'Droid Sans Fallback', 'AR PL UMing CN']

    # æ£€æµ‹ç³»ç»Ÿä¸­å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    available_fonts = set(f.name for f in font_manager.fontManager.ttflist)

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    chinese_font = None
    for font in font_list:
        if font in available_fonts:
            chinese_font = font
            break

    # å¦‚æœæ‰¾åˆ°äº†ä¸­æ–‡å­—ä½“ï¼Œè®¾ç½®å®ƒ
    if chinese_font:
        plt.rcParams['font.sans-serif'] = [chinese_font, 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        return chinese_font
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“
        # å¹¶è®¾ç½®ä¸ºä¸æ˜¾ç¤ºè´Ÿå·é—®é¢˜
        plt.rcParams['axes.unicode_minus'] = False

        # å°è¯•ä»æ‰€æœ‰å¯ç”¨å­—ä½“ä¸­æ‰¾CJKå­—ä½“
        cjk_fonts = [f.name for f in font_manager.fontManager.ttflist
                     if 'CJK' in f.name or 'Chinese' in f.name or
                     'SC' in f.name or 'CN' in f.name]

        if cjk_fonts:
            plt.rcParams['font.sans-serif'] = [cjk_fonts[0], 'DejaVu Sans']
            return cjk_fonts[0]
        else:
            # æœ€åçš„å¤‡é€‰ï¼šä½¿ç”¨é€šç”¨å­—ä½“ï¼Œä½†å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
            return 'DejaVu Sans (ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹æ¡†)'


# åœ¨ç¨‹åºå¼€å§‹æ—¶è®¾ç½®å­—ä½“
try:
    detected_font = setup_chinese_font()
    print(f"ä½¿ç”¨å­—ä½“: {detected_font}")
except Exception as e:
    print(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="RSEIè®¡ç®—ç³»ç»Ÿ",
    page_icon="ğŸŒ¿",
    layout="wide",
    initial_sidebar_state="expanded"
)


# =============================
# æ ¸å¿ƒè®¡ç®—ç±»
# =============================
class JenksNaturalBreaks:
    """ä¼˜åŒ–çš„Jenksè‡ªç„¶é—´æ–­ç‚¹åˆ†ç±»ç®—æ³•"""

    @staticmethod
    def calculate_jenks_breaks(data: np.ndarray, n_classes: int = 5,
                               max_samples: int = 5000) -> List[float]:
        valid_data = data[~np.isnan(data)].flatten()
        if len(valid_data) == 0:
            raise ValueError("æ•°æ®å…¨ä¸ºNaN")

        if len(valid_data) > max_samples:
            st.info(f"æ•°æ®é‡ {len(valid_data):,} è¿‡å¤§ï¼Œé‡‡æ ·è‡³ {max_samples:,} ä¸ªç‚¹...")
            valid_data = JenksNaturalBreaks._stratified_sample(valid_data, max_samples)

        methods = [
            ('jenkspy', JenksNaturalBreaks._jenks_by_jenkspy),
            ('numba', JenksNaturalBreaks._jenks_by_numba),
            ('optimized_numpy', JenksNaturalBreaks._jenks_optimized_numpy),
            ('fallback', JenksNaturalBreaks._jenks_fallback)
        ]

        for method_name, method_func in methods:
            try:
                breaks = method_func(valid_data, n_classes)
                if breaks is not None:
                    st.success(f"ä½¿ç”¨æ–¹æ³•: {method_name}")
                    return breaks
            except Exception as e:
                if method_name != 'fallback':
                    st.warning(f"{method_name} æ–¹æ³•å¤±è´¥: {e}")
                    continue
                else:
                    raise

        return JenksNaturalBreaks.get_default_breaks()

    @staticmethod
    def _stratified_sample(data: np.ndarray, n_samples: int) -> np.ndarray:
        sorted_data = np.sort(data)
        indices = np.linspace(0, len(sorted_data) - 1, n_samples, dtype=int)
        np.random.seed(42)
        noise = np.random.randint(-2, 3, size=n_samples)
        indices = np.clip(indices + noise, 0, len(sorted_data) - 1)
        return sorted_data[indices]

    @staticmethod
    def _jenks_by_jenkspy(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        try:
            import jenkspy
            breaks = jenkspy.jenks_breaks(data, n_classes=n_classes)
            return breaks[1:-1]
        except ImportError:
            return None

    @staticmethod
    def _jenks_by_numba(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        try:
            from numba import jit

            @jit(nopython=True)
            def _jenks_matrices_numba(data, n_classes):
                n_data = len(data)
                mat1 = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)
                mat2 = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)

                for i in range(1, n_classes + 1):
                    mat1[1, i] = 1
                    mat2[1, i] = 0
                    for j in range(2, n_data + 1):
                        mat2[j, i] = np.inf

                for l in range(2, n_data + 1):
                    s1 = 0.0
                    s2 = 0.0

                    for m in range(1, l + 1):
                        i3 = l - m + 1
                        val = data[i3 - 1]
                        s2 += val * val
                        s1 += val
                        w = m
                        v = s2 - (s1 * s1) / w
                        i4 = i3 - 1

                        if i4 != 0:
                            for j in range(2, n_classes + 1):
                                if mat2[l, j] >= (v + mat2[i4, j - 1]):
                                    mat1[l, j] = i3
                                    mat2[l, j] = v + mat2[i4, j - 1]

                    mat1[l, 1] = 1
                    mat2[l, 1] = v

                return mat1, mat2

            sorted_data = np.sort(data)
            mat1, mat2 = _jenks_matrices_numba(sorted_data, n_classes)

            k = len(sorted_data)
            kclass = []

            for j in range(n_classes, 0, -1):
                idx = int(mat1[k, j]) - 2
                if idx >= 0 and idx < len(sorted_data):
                    kclass.append(sorted_data[idx])
                k = int(mat1[k, j]) - 1

            kclass.reverse()
            if len(kclass) > 1:
                return kclass[1:][:n_classes - 1]
            else:
                return None

        except ImportError:
            return None

    @staticmethod
    def _jenks_optimized_numpy(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        sorted_data = np.sort(data)
        n_data = len(sorted_data)

        lower_class_limits = np.zeros((n_data + 1, n_classes + 1), dtype=np.int32)
        variance_combinations = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)
        variance_combinations[:, :] = np.inf

        lower_class_limits[1:, 1] = 1
        variance_combinations[1, :] = 0

        for l in range(2, n_data + 1):
            sum_val = 0.0
            sum_sq = 0.0

            for m in range(1, l + 1):
                lower_class_limit = l - m + 1
                val = sorted_data[lower_class_limit - 1]

                sum_val += val
                sum_sq += val * val
                w = m

                variance = sum_sq - (sum_val * sum_val) / w

                if lower_class_limit > 1:
                    for j in range(2, n_classes + 1):
                        if variance_combinations[l, j] >= (
                                variance + variance_combinations[lower_class_limit - 1, j - 1]):
                            lower_class_limits[l, j] = lower_class_limit
                            variance_combinations[l, j] = variance + variance_combinations[
                                lower_class_limit - 1, j - 1]

            lower_class_limits[l, 1] = 1
            variance_combinations[l, 1] = variance

        k = n_data
        breaks = []

        for j in range(n_classes, 1, -1):
            idx = lower_class_limits[k, j] - 2
            if 0 <= idx < len(sorted_data):
                breaks.append(float(sorted_data[idx]))
            k = lower_class_limits[k, j] - 1

        breaks.reverse()

        if len(breaks) >= n_classes - 1:
            return breaks[:n_classes - 1]
        else:
            return None

    @staticmethod
    def _jenks_fallback(data: np.ndarray, n_classes: int) -> List[float]:
        sorted_data = np.sort(data)
        n = len(sorted_data)
        breaks = []

        quantiles = np.linspace(0, 1, n_classes + 1)[1:-1]

        for q in quantiles:
            idx = int(q * n)
            window = min(int(n * 0.05), 100)
            start = max(0, idx - window)
            end = min(n, idx + window)

            if start >= end - 1:
                breaks.append(sorted_data[idx])
                continue

            min_variance = np.inf
            best_idx = idx

            for i in range(start, end):
                if i == 0 or i == n:
                    continue

                left_var = np.var(sorted_data[max(0, i - window):i]) if i > window else 0
                right_var = np.var(sorted_data[i:min(n, i + window)]) if i < n - window else 0
                total_var = left_var + right_var

                if total_var < min_variance:
                    min_variance = total_var
                    best_idx = i

            breaks.append(float(sorted_data[best_idx]))

        return breaks[:n_classes - 1]

    @staticmethod
    def get_default_breaks() -> List[float]:
        return [0.2, 0.4, 0.6, 0.8]


@dataclass
class BandConfig:
    blue: int = 1
    green: int = 2
    red: int = 3
    nir: int = 4
    swir1: int = 5
    swir2: int = 6
    tir: int = 7

    @classmethod
    def landsat8_stacked(cls):
        return cls(blue=1, green=2, red=3, nir=4, swir1=5, swir2=6, tir=7)

    @classmethod
    def sentinel2_stacked(cls):
        return cls(blue=1, green=2, red=3, nir=4, swir1=5, swir2=6, tir=None)


@dataclass
class RSEIConfig:
    satellite: str = 'Landsat8'
    band_config: BandConfig = None
    use_pca: bool = True
    export_indices: bool = True
    export_geotiff: bool = True
    mask_water: bool = True
    water_index: str = 'MNDWI'
    water_threshold: Optional[float] = None
    use_otsu: bool = True
    use_jenks: bool = True
    classification_breaks: List[float] = field(default_factory=lambda: [0.2, 0.4, 0.6, 0.8])
    jenks_samples: int = 5000

    def __post_init__(self):
        if self.band_config is None:
            if self.satellite.lower() == 'landsat8':
                self.band_config = BandConfig.landsat8_stacked()
            elif self.satellite.lower() == 'sentinel2':
                self.band_config = BandConfig.sentinel2_stacked()
            else:
                self.band_config = BandConfig()


class MultiSpectralImageReader:
    def __init__(self, config: RSEIConfig):
        self.config = config
        self.metadata = None
        self.bands_data = {}

    def read_multiband_tif(self, tif_path: str) -> Dict[str, np.ndarray]:
        st.info(f"ğŸ“¡ è¯»å–å¤šæ³¢æ®µå½±åƒ: {Path(tif_path).name}")

        with rasterio.open(tif_path) as src:
            st.write(f"å°ºå¯¸: {src.width} x {src.height}, æ³¢æ®µæ•°: {src.count}")

            self.metadata = {
                'transform': src.transform,
                'crs': src.crs,
                'width': src.width,
                'height': src.height,
                'dtype': 'float32',
                'count': 1,
                'driver': 'GTiff',
                'nodata': np.nan
            }

            required_bands = self._get_required_bands()
            max_band_idx = max([idx for idx in required_bands.values() if idx is not None])

            if src.count < max_band_idx:
                raise ValueError(f"æ³¢æ®µæ•°é‡ä¸è¶³ï¼éœ€è¦{max_band_idx}ä¸ªï¼Œå®é™…{src.count}ä¸ª")

            bands = {}
            for band_name, band_idx in required_bands.items():
                if band_idx is None:
                    continue

                band_data = src.read(band_idx).astype(float)
                if src.nodata is not None:
                    band_data[band_data == src.nodata] = np.nan
                band_data[band_data < -9999] = np.nan
                band_data[band_data > 50000] = np.nan
                bands[band_name] = band_data

        st.success(f"âœ… æˆåŠŸè¯»å– {len(bands)} ä¸ªæ³¢æ®µ")
        self.bands_data = bands
        return bands

    def _get_required_bands(self) -> Dict[str, int]:
        bc = self.config.band_config
        return {
            'blue': bc.blue, 'green': bc.green, 'red': bc.red,
            'nir': bc.nir, 'swir1': bc.swir1, 'swir2': bc.swir2,
            'tir': bc.tir
        }

    def apply_scale_factor(self, bands: Dict[str, np.ndarray],
                           scale_factor: float = 0.0001) -> Dict[str, np.ndarray]:
        st.info(f"ğŸ”§ åº”ç”¨ç¼©æ”¾å› å­: {scale_factor}")
        scaled_bands = {}
        optical_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']

        for band_name, band_data in bands.items():
            if band_name in optical_bands:
                scaled_bands[band_name] = band_data * scale_factor
            else:
                scaled_bands[band_name] = band_data
        return scaled_bands


class OTSUThreshold:
    @staticmethod
    def calculate_otsu_threshold(data: np.ndarray, bins: int = 256) -> Tuple[float, Dict]:
        valid_data = data[~np.isnan(data)]
        if len(valid_data) == 0:
            raise ValueError("æ•°æ®å…¨ä¸ºNaN")

        hist, bin_edges = np.histogram(valid_data, bins=bins)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        hist = hist.astype(float)
        prob = hist / hist.sum()

        max_variance = 0
        optimal_threshold = 0

        w0 = np.cumsum(prob)
        w1 = 1 - w0
        mu = np.cumsum(prob * bin_centers)
        mu_total = mu[-1]

        for i in range(1, bins):
            if w0[i] == 0 or w1[i] == 0:
                continue

            mu0 = mu[i] / w0[i] if w0[i] > 0 else 0
            mu1 = (mu_total - mu[i]) / w1[i] if w1[i] > 0 else 0
            variance = w0[i] * w1[i] * (mu0 - mu1) ** 2

            if variance > max_variance:
                max_variance = variance
                optimal_threshold = bin_centers[i]

        threshold_idx = np.argmin(np.abs(bin_centers - optimal_threshold))

        metrics = {
            'threshold': optimal_threshold,
            'max_variance': max_variance,
            'histogram': hist,
            'bin_centers': bin_centers,
            'threshold_idx': threshold_idx,
            'background_prob': w0[threshold_idx],
            'foreground_prob': w1[threshold_idx]
        }

        return optimal_threshold, metrics


class WaterMaskGenerator:
    @staticmethod
    def calculate_mndwi(green: np.ndarray, swir1: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            mndwi = (green - swir1) / (green + swir1)
        return mndwi

    @staticmethod
    def calculate_ndwi(green: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndwi = (green - nir) / (green + nir)
        return ndwi

    @staticmethod
    def calculate_aweish(blue: np.ndarray, green: np.ndarray,
                         nir: np.ndarray, swir1: np.ndarray,
                         swir2: np.ndarray) -> np.ndarray:
        aweish = blue + 2.5 * green - 1.5 * (nir + swir1) - 0.25 * swir2
        return aweish

    @staticmethod
    def create_water_mask(bands: Dict[str, np.ndarray],
                          method: str = 'MNDWI',
                          threshold: Optional[float] = None,
                          use_otsu: bool = True) -> Tuple[np.ndarray, np.ndarray, float]:
        st.info(f"ğŸ’§ åˆ›å»ºæ°´ä½“æ©è†œ (æ–¹æ³•: {method})")

        if method.upper() == 'NDWI':
            water_index = WaterMaskGenerator.calculate_ndwi(bands['green'], bands['nir'])
        elif method.upper() == 'MNDWI':
            water_index = WaterMaskGenerator.calculate_mndwi(bands['green'], bands['swir1'])
        elif method.upper() == 'AWEISH':
            water_index = WaterMaskGenerator.calculate_aweish(
                bands['blue'], bands['green'], bands['nir'],
                bands['swir1'], bands['swir2']
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ°´ä½“æŒ‡æ•°: {method}")

        if use_otsu and threshold is None:
            try:
                otsu_threshold, metrics = OTSUThreshold.calculate_otsu_threshold(water_index, bins=256)
                final_threshold = otsu_threshold
                st.success(f"âœ“ OTSUé˜ˆå€¼: {final_threshold:.4f}")
            except Exception as e:
                st.warning(f"âš ï¸ OTSUå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é˜ˆå€¼0.0")
                final_threshold = 0.0
        elif threshold is not None:
            final_threshold = threshold
        else:
            final_threshold = 0.0

        water_mask = water_index > final_threshold

        total_pixels = np.sum(~np.isnan(water_index))
        water_pixels = np.sum(water_mask)
        water_ratio = water_pixels / total_pixels * 100 if total_pixels > 0 else 0

        st.write(f"æ°´åŸŸ: {water_pixels:,} ({water_ratio:.2f}%)")

        return water_index, water_mask, final_threshold


class RemoteSensingIndices:
    @staticmethod
    def calculate_ndvi(red: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndvi = (nir - red) / (nir + red)
        return np.clip(ndvi, -1, 1)

    @staticmethod
    def calculate_ndbi(swir1: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndbi = (swir1 - nir) / (swir1 + nir)
        return ndbi

    @staticmethod
    def calculate_ndbsi(bands: Dict[str, np.ndarray]) -> np.ndarray:
        swir1, red, nir, blue = bands['swir1'], bands['red'], bands['nir'], bands['blue']
        with np.errstate(divide='ignore', invalid='ignore'):
            si = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue))
            ndbi = RemoteSensingIndices.calculate_ndbi(swir1, nir)
            ndbsi = (si + ndbi) / 2
        return ndbsi

    @staticmethod
    def calculate_wet(bands: Dict[str, np.ndarray], sensor: str = 'Landsat8') -> np.ndarray:
        coeffs = {
            'blue': 0.1511, 'green': 0.1973, 'red': 0.3283,
            'nir': 0.3407, 'swir1': -0.7117, 'swir2': -0.4559
        }
        wet = np.zeros_like(bands['blue'])
        for band_name, coeff in coeffs.items():
            if band_name in bands:
                wet += coeff * bands[band_name]
        return wet

    @staticmethod
    def calculate_lst_simple(tir: np.ndarray) -> np.ndarray:
        if np.nanmax(tir) > 400:
            ML = 3.3420E-04
            AL = 0.10000
            K1 = 774.8853
            K2 = 1321.0789
            L_lambda = ML * tir + AL
            with np.errstate(divide='ignore', invalid='ignore'):
                T_b = K2 / np.log(K1 / L_lambda + 1)
        else:
            T_b = tir
        return T_b - 273.15


class RSEICalculator:
    def __init__(self, config: RSEIConfig = None):
        self.config = config or RSEIConfig()
        self.indices = {}
        self.rsei = None
        self.rsei_components = None
        self.calculated_breaks = None
        self.jenks_time = 0

    def normalize(self, array: np.ndarray, inverse: bool = False) -> np.ndarray:
        valid_mask = ~np.isnan(array)
        if not valid_mask.any():
            return array

        min_val = np.nanmin(array)
        max_val = np.nanmax(array)

        if max_val == min_val:
            return np.ones_like(array) * 0.5

        normalized = (array - min_val) / (max_val - min_val)
        if inverse:
            normalized = 1 - normalized
        return normalized

    def apply_water_mask(self, array: np.ndarray, water_mask: np.ndarray) -> np.ndarray:
        masked_array = array.copy()
        masked_array[water_mask] = np.nan
        return masked_array

    def calculate_rsei_pca(self, greenness: np.ndarray, wetness: np.ndarray,
                           dryness: np.ndarray, heat: np.ndarray,
                           water_mask: Optional[np.ndarray] = None) -> np.ndarray:
        st.info("ğŸ”¬ è®¡ç®—RSEI (PCAæ–¹æ³•)...")

        if water_mask is not None:
            greenness = self.apply_water_mask(greenness, water_mask)
            wetness = self.apply_water_mask(wetness, water_mask)
            dryness = self.apply_water_mask(dryness, water_mask)
            heat = self.apply_water_mask(heat, water_mask)

        green_norm = self.normalize(greenness, inverse=False)
        wet_norm = self.normalize(wetness, inverse=False)
        dry_norm = self.normalize(dryness, inverse=True)
        heat_norm = self.normalize(heat, inverse=True)

        mask = ~(np.isnan(green_norm) | np.isnan(wet_norm) |
                 np.isnan(dry_norm) | np.isnan(heat_norm))

        n_valid = mask.sum()
        st.write(f"æœ‰æ•ˆåƒç´ : {n_valid:,}")

        if n_valid < 100:
            raise ValueError("æœ‰æ•ˆåƒç´ å¤ªå°‘")

        data_matrix = np.column_stack([
            green_norm[mask], wet_norm[mask],
            dry_norm[mask], heat_norm[mask]
        ])

        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data_matrix)

        pca = PCA(n_components=4)
        pc_all = pca.fit_transform(data_scaled)
        pc1 = pc_all[:, 0]

        rsei_raw = np.full(greenness.shape, np.nan)
        rsei_raw[mask] = pc1.flatten()
        rsei = self.normalize(rsei_raw, inverse=False)

        st.success(f"PC1è´¡çŒ®ç‡: {pca.explained_variance_ratio_[0] * 100:.2f}%")

        self.rsei_components = {
            'greenness': green_norm,
            'wetness': wet_norm,
            'dryness': dry_norm,
            'heat': heat_norm,
            'pca': {
                'variance_ratio': pca.explained_variance_ratio_.tolist(),
                'components': pca.components_.tolist()
            }
        }

        if self.config.use_jenks:
            st.info("ğŸ” è®¡ç®—Jenksè‡ªç„¶é—´æ–­ç‚¹é˜ˆå€¼...")
            start_time = time.time()
            try:
                breaks = JenksNaturalBreaks.calculate_jenks_breaks(
                    rsei,
                    n_classes=5,
                    max_samples=self.config.jenks_samples
                )
                self.calculated_breaks = breaks
                self.jenks_time = time.time() - start_time
                st.success(f"âœ“ Jenksé˜ˆå€¼: {[f'{b:.4f}' for b in breaks]}")
                st.success(f"âœ“ è®¡ç®—è€—æ—¶: {self.jenks_time:.2f}ç§’")
            except Exception as e:
                st.warning(f"âš ï¸ Jenksè®¡ç®—å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é˜ˆå€¼")
                self.calculated_breaks = JenksNaturalBreaks.get_default_breaks()
                self.jenks_time = 0
        else:
            self.calculated_breaks = self.config.classification_breaks

        return rsei

    def classify_rsei(self, rsei: np.ndarray, breaks: Optional[List[float]] = None) -> np.ndarray:
        if breaks is None:
            breaks = self.calculated_breaks or self.config.classification_breaks

        classified = np.full_like(rsei, np.nan)

        if len(breaks) < 4:
            breaks = JenksNaturalBreaks.get_default_breaks()

        t1, t2, t3, t4 = breaks[0], breaks[1], breaks[2], breaks[3]

        classified[rsei < t1] = 1
        classified[(rsei >= t1) & (rsei < t2)] = 2
        classified[(rsei >= t2) & (rsei < t3)] = 3
        classified[(rsei >= t3) & (rsei < t4)] = 4
        classified[rsei >= t4] = 5

        return classified


class RSEIVisualizer:
    @staticmethod
    def create_comprehensive_visualization(rsei, rsei_class, indices,
                                           water_index=None, water_threshold=None,
                                           classification_breaks=None):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾ - ä¿®å¤ä¸­æ–‡æ˜¾ç¤º"""

        rsei_colors = ['#d73027', '#fc8d59', '#fee08b', '#91cf60', '#1a9850']
        rsei_cmap = ListedColormap(rsei_colors)

        # åˆ›å»ºå›¾å½¢ï¼Œå¢å¤§å­—ä½“ä»¥ç¡®ä¿æ˜¾ç¤º
        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)

        # 1. RSEIè¿ç»­å€¼
        ax1 = fig.add_subplot(gs[0, 0])
        im1 = ax1.imshow(rsei, cmap='RdYlGn', vmin=0, vmax=1)
        ax1.set_title('RSEI (è¿ç»­å€¼)', fontsize=12, fontweight='bold')
        ax1.axis('off')
        plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)

        # 2. RSEIåˆ†ç±»
        ax2 = fig.add_subplot(gs[0, 1])
        im2 = ax2.imshow(rsei_class, cmap=rsei_cmap, vmin=1, vmax=5)

        if classification_breaks:
            breaks_str = f"[{classification_breaks[0]:.2f}, {classification_breaks[1]:.2f}, " \
                         f"{classification_breaks[2]:.2f}, {classification_breaks[3]:.2f}]"
            ax2.set_title(f'RSEIåˆ†ç±»\né˜ˆå€¼: {breaks_str}', fontsize=10, fontweight='bold')
        else:
            ax2.set_title('RSEIåˆ†ç±»', fontsize=12, fontweight='bold')
        ax2.axis('off')

        class_names = ['å·®', 'è¾ƒå·®', 'ä¸­ç­‰', 'è‰¯å¥½', 'ä¼˜ç§€']
        legend_elements = [Patch(facecolor=rsei_colors[i], label=class_names[i])
                           for i in range(5)]
        ax2.legend(handles=legend_elements, loc='upper right', fontsize=8, prop={'size': 9})

        # 3-6. å››ä¸ªæŒ‡æ•°
        indices_to_plot = [
            ('ndvi', 'NDVI (ç»¿åº¦)', 'Greens'),
            ('wet', 'WET (æ¹¿åº¦)', 'Blues'),
            ('ndbsi', 'NDBSI (å¹²åº¦)', 'YlOrRd'),
            ('lst', 'LST (çƒ­åº¦)', 'hot')
        ]

        for idx, (key, title, cmap) in enumerate(indices_to_plot):
            ax = fig.add_subplot(gs[0, 2 + idx % 2] if idx < 2 else gs[1, idx - 2])
            if key in indices:
                im = ax.imshow(indices[key], cmap=cmap)
                ax.set_title(title, fontsize=12, fontweight='bold')
                ax.axis('off')
                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

        # 7. æ°´ä½“æ©è†œ
        if water_index is not None:
            ax7 = fig.add_subplot(gs[1, 2])
            im7 = ax7.imshow(water_index, cmap='RdYlBu')
            ax7.set_title(f'æ°´ä½“æŒ‡æ•° (é˜ˆå€¼={water_threshold:.3f})', fontsize=12, fontweight='bold')
            ax7.axis('off')
            plt.colorbar(im7, ax=ax7, fraction=0.046, pad=0.04)

        # 8. RSEIç»Ÿè®¡ç›´æ–¹å›¾
        ax8 = fig.add_subplot(gs[1, 3])
        valid_rsei = rsei[~np.isnan(rsei)]
        ax8.hist(valid_rsei, bins=50, color='steelblue', edgecolor='black', alpha=0.7)
        ax8.axvline(np.nanmean(rsei), color='red', linestyle='--',
                    label=f'å‡å€¼={np.nanmean(rsei):.3f}', linewidth=2)

        if classification_breaks:
            colors_line = ['#d73027', '#fc8d59', '#fee08b', '#91cf60']
            for i, threshold in enumerate(classification_breaks):
                ax8.axvline(threshold, color=colors_line[i], linestyle=':',
                            linewidth=1.5, alpha=0.7)

        ax8.set_title('RSEIåˆ†å¸ƒ', fontsize=12, fontweight='bold')
        ax8.set_xlabel('RSEIå€¼', fontsize=10)
        ax8.set_ylabel('é¢‘æ•°', fontsize=10)
        ax8.legend(fontsize=8, prop={'size': 9})
        ax8.grid(alpha=0.3)

        # 9. ç­‰çº§é¢ç§¯ç»Ÿè®¡
        ax9 = fig.add_subplot(gs[2, :2])
        class_counts = [np.sum(rsei_class == i) for i in range(1, 6)]
        colors = rsei_colors
        bars = ax9.bar(class_names, class_counts, color=colors, edgecolor='black', alpha=0.8)
        ax9.set_title('RSEIç­‰çº§é¢ç§¯ç»Ÿè®¡', fontsize=12, fontweight='bold')
        ax9.set_ylabel('åƒç´ æ•°é‡', fontsize=10)
        ax9.grid(axis='y', alpha=0.3)

        for bar, count in zip(bars, class_counts):
            height = bar.get_height()
            ax9.text(bar.get_x() + bar.get_width() / 2., height,
                     f'{int(count):,}\n({count / np.sum(class_counts) * 100:.1f}%)',
                     ha='center', va='bottom', fontsize=9)

        # 10. ç­‰çº§é¢ç§¯é¥¼å›¾
        ax10 = fig.add_subplot(gs[2, 2:])
        wedges, texts, autotexts = ax10.pie(class_counts, labels=class_names,
                                            colors=colors, autopct='%1.1f%%',
                                            startangle=90, textprops={'fontsize': 10})
        ax10.set_title('RSEIç­‰çº§æ¯”ä¾‹', fontsize=12, fontweight='bold')

        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
            autotext.set_fontsize(10)

        title_text = 'RSEIç»¼åˆåˆ†æç»“æœ'
        if classification_breaks:
            method = "Jenksè‡ªç„¶é—´æ–­ç‚¹" if classification_breaks != [0.2, 0.4, 0.6, 0.8] else "ç­‰é—´è·"
            title_text += f' ({method}åˆ†ç±»)'
        fig.suptitle(title_text, fontsize=16, fontweight='bold', y=0.98)

        # ç¡®ä¿å­—ä½“æ¸²æŸ“æ­£ç¡®
        plt.tight_layout()

        return fig


# =============================
# æ ¸å¿ƒè®¡ç®—å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼Œä»£ç å¤ªé•¿çœç•¥...ï¼‰
# =============================
def execute_rsei_calculation(input_file, config):
    """æ ¸å¿ƒè®¡ç®—é€»è¾‘ - å¢å¼ºç‰ˆï¼Œå¯¼å‡ºæ‰€æœ‰æŒ‡æ•°"""
    temp_dir = tempfile.mkdtemp()
    output_path = Path(temp_dir)

    try:
        progress_bar = st.progress(0)
        status_text = st.empty()

        # 1. è¯»å–å½±åƒ
        status_text.text("æ­¥éª¤1/9: è¯»å–å½±åƒ...")
        progress_bar.progress(10)
        reader = MultiSpectralImageReader(config)
        bands = reader.read_multiband_tif(input_file)

        # 2. é¢„å¤„ç†
        status_text.text("æ­¥éª¤2/9: æ•°æ®é¢„å¤„ç†...")
        progress_bar.progress(15)
        max_val = np.nanmax(bands['red'])
        if max_val > 1.0:
            bands = reader.apply_scale_factor(bands, 0.0001)

        # 3. æ°´ä½“æ©è†œ
        water_index = None
        water_mask = None
        water_threshold_used = None

        if config.mask_water:
            status_text.text("æ­¥éª¤3/9: åˆ›å»ºæ°´ä½“æ©è†œ...")
            progress_bar.progress(25)
            water_index, water_mask, water_threshold_used = WaterMaskGenerator.create_water_mask(
                bands, config.water_index, config.water_threshold, config.use_otsu
            )

        # 4. è®¡ç®—æŒ‡æ•°
        status_text.text("æ­¥éª¤4/9: è®¡ç®—é¥æ„ŸæŒ‡æ•°...")
        progress_bar.progress(35)
        calc = RemoteSensingIndices()
        ndvi = calc.calculate_ndvi(bands['red'], bands['nir'])
        wet = calc.calculate_wet(bands, config.satellite)
        ndbsi = calc.calculate_ndbsi(bands)

        ndbi = calc.calculate_ndbi(bands['swir1'], bands['nir'])
        swir1, red, nir, blue = bands['swir1'], bands['red'], bands['nir'], bands['blue']
        with np.errstate(divide='ignore', invalid='ignore'):
            si = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue))

        if 'tir' in bands and bands['tir'] is not None:
            lst = calc.calculate_lst_simple(bands['tir'])
        else:
            lst = ndbsi

        indices = {
            'ndvi': ndvi,
            'wet': wet,
            'ndbsi': ndbsi,
            'lst': lst,
            'ndbi': ndbi,
            'si': si
        }

        # 5. è®¡ç®—RSEI
        status_text.text("æ­¥éª¤5/9: è®¡ç®—RSEI...")
        progress_bar.progress(45)
        rsei_calc = RSEICalculator(config)
        rsei = rsei_calc.calculate_rsei_pca(ndvi, wet, ndbsi, lst, water_mask)

        # 6. åˆ†ç±»
        status_text.text("æ­¥éª¤6/9: RSEIåˆ†ç±»...")
        progress_bar.progress(55)
        classification_breaks = rsei_calc.calculated_breaks
        rsei_class = rsei_calc.classify_rsei(rsei, classification_breaks)

        # ç»Ÿè®¡
        class_names = ['å·®', 'è¾ƒå·®', 'ä¸­ç­‰', 'è‰¯å¥½', 'ä¼˜ç§€']
        total_valid = np.sum(~np.isnan(rsei_class))

        st.write(f"\nä½¿ç”¨çš„åˆ†ç±»é˜ˆå€¼: {[f'{b:.4f}' for b in classification_breaks]}")
        st.write("\nç­‰çº§åˆ†å¸ƒ:")
        for i, name in enumerate(class_names, 1):
            count = np.sum(rsei_class == i)
            ratio = count / total_valid * 100 if total_valid > 0 else 0
            st.write(f"{name}: {count:,} ({ratio:.2f}%)")

        # 7. ç”Ÿæˆå¯è§†åŒ–
        status_text.text("æ­¥éª¤7/9: ç”Ÿæˆå¯è§†åŒ–å›¾...")
        progress_bar.progress(65)
        fig = RSEIVisualizer.create_comprehensive_visualization(
            rsei, rsei_class, indices, water_index,
            water_threshold_used, classification_breaks
        )

        img_path = output_path / 'RSEI_comprehensive.png'
        fig.savefig(img_path, dpi=300, bbox_inches='tight')
        plt.close(fig)

        # 8. å¯¼å‡ºGeoTIFFæ–‡ä»¶
        status_text.text("æ­¥éª¤8/9: å¯¼å‡ºGeoTIFFæ–‡ä»¶...")
        progress_bar.progress(75)

        saved_files = []

        if config.export_geotiff and reader.metadata:
            with rasterio.open(output_path / 'RSEI.tif', 'w', **reader.metadata) as dst:
                dst.write(rsei.astype('float32'), 1)
            saved_files.append('RSEI.tif')

            with rasterio.open(output_path / 'RSEI_classified.tif', 'w', **reader.metadata) as dst:
                dst.write(rsei_class.astype('float32'), 1)
            saved_files.append('RSEI_classified.tif')

            if water_index is not None:
                with rasterio.open(output_path / 'Water_Index.tif', 'w', **reader.metadata) as dst:
                    dst.write(water_index.astype('float32'), 1)
                saved_files.append('Water_Index.tif')

                with rasterio.open(output_path / 'Water_Mask.tif', 'w', **reader.metadata) as dst:
                    dst.write(water_mask.astype('float32'), 1)
                saved_files.append('Water_Mask.tif')

            if config.export_indices:
                st.info("æ­£åœ¨å¯¼å‡ºæ‰€æœ‰é¥æ„ŸæŒ‡æ•°...")

                with rasterio.open(output_path / 'NDVI.tif', 'w', **reader.metadata) as dst:
                    dst.write(ndvi.astype('float32'), 1)
                saved_files.append('NDVI.tif')

                with rasterio.open(output_path / 'WET.tif', 'w', **reader.metadata) as dst:
                    dst.write(wet.astype('float32'), 1)
                saved_files.append('WET.tif')

                with rasterio.open(output_path / 'NDBSI.tif', 'w', **reader.metadata) as dst:
                    dst.write(ndbsi.astype('float32'), 1)
                saved_files.append('NDBSI.tif')

                with rasterio.open(output_path / 'LST.tif', 'w', **reader.metadata) as dst:
                    dst.write(lst.astype('float32'), 1)
                saved_files.append('LST.tif')

                with rasterio.open(output_path / 'NDBI.tif', 'w', **reader.metadata) as dst:
                    dst.write(ndbi.astype('float32'), 1)
                saved_files.append('NDBI.tif')

                with rasterio.open(output_path / 'SI.tif', 'w', **reader.metadata) as dst:
                    dst.write(si.astype('float32'), 1)
                saved_files.append('SI.tif')

                greenness = rsei_calc.rsei_components['greenness']
                wetness_norm = rsei_calc.rsei_components['wetness']
                dryness = rsei_calc.rsei_components['dryness']
                heat = rsei_calc.rsei_components['heat']

                with rasterio.open(output_path / 'Greenness_Normalized.tif', 'w', **reader.metadata) as dst:
                    dst.write(greenness.astype('float32'), 1)
                saved_files.append('Greenness_Normalized.tif')

                with rasterio.open(output_path / 'Wetness_Normalized.tif', 'w', **reader.metadata) as dst:
                    dst.write(wetness_norm.astype('float32'), 1)
                saved_files.append('Wetness_Normalized.tif')

                with rasterio.open(output_path / 'Dryness_Normalized.tif', 'w', **reader.metadata) as dst:
                    dst.write(dryness.astype('float32'), 1)
                saved_files.append('Dryness_Normalized.tif')

                with rasterio.open(output_path / 'Heat_Normalized.tif', 'w', **reader.metadata) as dst:
                    dst.write(heat.astype('float32'), 1)
                saved_files.append('Heat_Normalized.tif')

        # 9. Excelç»Ÿè®¡
        status_text.text("æ­¥éª¤9/9: ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        progress_bar.progress(85)

        stats_df = pd.DataFrame({
            'æŒ‡æ ‡': ['NDVI', 'WET', 'NDBSI', 'LST', 'NDBI', 'SI', 'RSEI'],
            'æœ€å°å€¼': [f"{np.nanmin(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'æœ€å¤§å€¼': [f"{np.nanmax(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'å‡å€¼': [f"{np.nanmean(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'æ ‡å‡†å·®': [f"{np.nanstd(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]]
        })

        class_df = pd.DataFrame({
            'ç­‰çº§': class_names,
            'åƒç´ æ•°': [int(np.sum(rsei_class == i)) for i in range(1, 6)],
            'ç™¾åˆ†æ¯”': [f"{np.sum(rsei_class == i) / total_valid * 100:.2f}%" for i in range(1, 6)]
        })

        threshold_df = pd.DataFrame({
            'åˆ†ç±»é˜ˆå€¼': ['å·®/è¾ƒå·®', 'è¾ƒå·®/ä¸­ç­‰', 'ä¸­ç­‰/è‰¯å¥½', 'è‰¯å¥½/ä¼˜ç§€'],
            'é˜ˆå€¼': [f"{b:.4f}" for b in classification_breaks],
            'æ–¹æ³•': ['Jenksè‡ªç„¶é—´æ–­ç‚¹' if config.use_jenks else 'æ‰‹åŠ¨è®¾ç½®'] * 4,
            'Jenksè€—æ—¶(ç§’)': [f"{rsei_calc.jenks_time:.2f}" if config.use_jenks else 'N/A'] * 4
        })

        files_df = pd.DataFrame({
            'æ–‡ä»¶å': saved_files,
            'è¯´æ˜': [
                'RSEIè¿ç»­å€¼ï¼ˆ0-1ï¼‰',
                'RSEIåˆ†ç±»ï¼ˆ1-5ï¼‰',
                'æ°´ä½“æŒ‡æ•°',
                'æ°´ä½“æ©è†œ',
                'å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•°',
                'æ¹¿åº¦æŒ‡æ•°',
                'å½’ä¸€åŒ–å»ºç­‘-åœŸå£¤æŒ‡æ•°',
                'åœ°è¡¨æ¸©åº¦',
                'å½’ä¸€åŒ–å»ºç­‘æŒ‡æ•°',
                'åœŸå£¤æŒ‡æ•°',
                'ç»¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'æ¹¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'å¹²åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'çƒ­åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰'
            ][:len(saved_files)]
        })

        excel_path = output_path / 'RSEI_analysis.xlsx'
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            stats_df.to_excel(writer, sheet_name='æŒ‡æ ‡ç»Ÿè®¡', index=False)
            class_df.to_excel(writer, sheet_name='ç­‰çº§åˆ†å¸ƒ', index=False)
            threshold_df.to_excel(writer, sheet_name='åˆ†ç±»é˜ˆå€¼', index=False)
            files_df.to_excel(writer, sheet_name='æ–‡ä»¶æ¸…å•', index=False)

        progress_bar.progress(100)
        status_text.text("âœ… è®¡ç®—å®Œæˆï¼")

        return {
            'rsei': rsei,
            'rsei_class': rsei_class,
            'indices': indices,
            'stats_df': stats_df,
            'class_df': class_df,
            'threshold_df': threshold_df,
            'files_df': files_df,
            'img_path': str(img_path),
            'excel_path': str(excel_path),
            'output_path': output_path,
            'classification_breaks': classification_breaks,
            'saved_files': saved_files
        }

    except Exception as e:
        st.error(f"è®¡ç®—å¤±è´¥: {e}")
        import traceback
        st.error(traceback.format_exc())
        return None


# =============================
# Streamlitä¸»ç¨‹åºï¼ˆå®Œæ•´ä¿ç•™ï¼‰
# =============================
def main():
    # æ˜¾ç¤ºå­—ä½“ä¿¡æ¯
    with st.sidebar:
        with st.expander("ğŸ”§ ç³»ç»Ÿä¿¡æ¯"):
            st.write(f"**æ“ä½œç³»ç»Ÿ:** {platform.system()}")
            current_font = plt.rcParams['font.sans-serif'][0]
            st.write(f"**ä½¿ç”¨å­—ä½“:** {current_font}")

            # æ£€æµ‹å¯ç”¨ä¸­æ–‡å­—ä½“
            available_fonts = set(f.name for f in font_manager.fontManager.ttflist)
            chinese_fonts = [f for f in available_fonts if any(
                keyword in f for keyword in
                ['Chinese', 'CJK', 'SC', 'CN', 'Hei', 'Song', 'Kai', 'PingFang', 'Microsoft', 'SimHei']
            )]
            if chinese_fonts:
                st.write(f"**å¯ç”¨ä¸­æ–‡å­—ä½“æ•°:** {len(chinese_fonts)}")
            else:
                st.warning("âš ï¸ æœªæ£€æµ‹åˆ°ä¸­æ–‡å­—ä½“")

    # é¡µé¢æ ‡é¢˜
    st.title("ğŸŒ¿ RSEIè®¡ç®—ç³»ç»Ÿ v3.7 - ä¿®å¤ä¸­æ–‡æ˜¾ç¤º")
    st.markdown("**Remote Sensing based Ecological Index é¥æ„Ÿç”Ÿæ€æŒ‡æ•°è®¡ç®—å·¥å…·**")

    # ä¾§è¾¹æ é…ç½®ï¼ˆå®Œæ•´ä¿ç•™å‰é¢çš„ä»£ç ...ï¼‰
    with st.sidebar:
        st.header("âš™ï¸ å‚æ•°é…ç½®")

        st.subheader("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
        uploaded_file = st.file_uploader(
            "é€‰æ‹©å¤šæ³¢æ®µTIFå½±åƒ",
            type=['tif', 'tiff'],
            help="ä¸Šä¼ Landsat 8æˆ–Sentinel-2çš„å¤šæ³¢æ®µå½±åƒæ–‡ä»¶"
        )

        st.subheader("ğŸ›°ï¸ å«æ˜Ÿå‚æ•°")
        satellite = st.selectbox("å«æ˜Ÿç±»å‹", ["Landsat8", "Sentinel2"], index=0)

        st.subheader("ğŸ”¬ è®¡ç®—æ–¹æ³•")
        use_pca = st.checkbox("ä½¿ç”¨PCAæ–¹æ³•", value=True)

        st.subheader("ğŸ“Š åˆ†ç±»é˜ˆå€¼è®¾ç½®")
        use_jenks = st.checkbox("ä½¿ç”¨Jenksè‡ªç„¶é—´æ–­ç‚¹", value=True)

        if use_jenks:
            jenks_samples = st.slider("é‡‡æ ·æ•°é‡", 1000, 20000, 5000, 1000)
        else:
            st.write("æ‰‹åŠ¨è®¾ç½®é˜ˆå€¼:")
            threshold_1 = st.number_input("å·®/è¾ƒå·®", 0.0, 1.0, 0.2, 0.01)
            threshold_2 = st.number_input("è¾ƒå·®/ä¸­ç­‰", 0.0, 1.0, 0.4, 0.01)
            threshold_3 = st.number_input("ä¸­ç­‰/è‰¯å¥½", 0.0, 1.0, 0.6, 0.01)
            threshold_4 = st.number_input("è‰¯å¥½/ä¼˜ç§€", 0.0, 1.0, 0.8, 0.01)

        st.subheader("ğŸ’§ æ°´ä½“æ©è†œ")
        mask_water = st.checkbox("å»é™¤æ°´åŸŸ", value=True)

        if mask_water:
            water_index = st.selectbox("æ°´ä½“æŒ‡æ•°", ["MNDWI", "NDWI", "AWEIsh"], index=0)
            use_otsu = st.checkbox("ä½¿ç”¨OTSUè‡ªåŠ¨è®¡ç®—é˜ˆå€¼", value=True)
            if not use_otsu:
                water_threshold = st.number_input("æ‰‹åŠ¨é˜ˆå€¼", -1.0, 1.0, 0.0, 0.1)
            else:
                water_threshold = None
        else:
            water_index = "MNDWI"
            use_otsu = True
            water_threshold = None

        st.subheader("ğŸ’¾ å¯¼å‡ºé€‰é¡¹")
        export_geotiff = st.checkbox("å¯¼å‡ºGeoTIFFæ–‡ä»¶", value=True)
        export_indices = st.checkbox("å¯¼å‡ºæ‰€æœ‰é¥æ„ŸæŒ‡æ•°", value=True)

    # ä¸»å†…å®¹åŒºï¼ˆç»§ç»­ä½¿ç”¨å‰é¢çš„å®Œæ•´ä»£ç ï¼‰
    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.tif') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        st.success(f"âœ… æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")
        file_size = len(uploaded_file.getvalue()) / (1024 * 1024)
        st.info(f"ğŸ“¦ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

        if st.button("â–¶ï¸ å¼€å§‹è®¡ç®—", type="primary"):
            if not use_jenks:
                thresholds = [threshold_1, threshold_2, threshold_3, threshold_4]
                if not all(thresholds[i] < thresholds[i + 1] for i in range(3)):
                    st.error("âŒ é˜ˆå€¼å¿…é¡»é€’å¢ï¼")
                    return

            config = RSEIConfig(
                satellite=satellite,
                use_pca=use_pca,
                export_indices=export_indices,
                export_geotiff=export_geotiff,
                mask_water=mask_water,
                water_index=water_index,
                water_threshold=water_threshold,
                use_otsu=use_otsu,
                use_jenks=use_jenks,
                classification_breaks=[threshold_1, threshold_2, threshold_3,
                                       threshold_4] if not use_jenks else [0.2, 0.4, 0.6, 0.8],
                jenks_samples=jenks_samples if use_jenks else 5000
            )

            start_time = time.time()

            with st.spinner("è®¡ç®—ä¸­ï¼Œè¯·ç¨å€™..."):
                results = execute_rsei_calculation(tmp_file_path, config)

            elapsed_time = time.time() - start_time

            if results:
                st.success(f"âœ… è®¡ç®—å®Œæˆï¼è€—æ—¶: {elapsed_time:.1f}ç§’")

                st.header("ğŸ“Š è®¡ç®—ç»“æœ")

                tab1, tab2, tab3, tab4, tab5 = st.tabs([
                    "ğŸ“ˆ ç»Ÿè®¡æ•°æ®",
                    "ğŸ–¼ï¸ å¯è§†åŒ–ç»“æœ",
                    "ğŸ“¥ ä¸‹è½½æ–‡ä»¶",
                    "ğŸ“‹ æ–‡ä»¶æ¸…å•",
                    "â„¹ï¸ è¯¦ç»†ä¿¡æ¯"
                ])

                with tab1:
                    st.subheader("æŒ‡æ ‡ç»Ÿè®¡")
                    st.dataframe(results['stats_df'], use_container_width=True)
                    st.subheader("ç­‰çº§åˆ†å¸ƒ")
                    st.dataframe(results['class_df'], use_container_width=True)
                    st.subheader("åˆ†ç±»é˜ˆå€¼")
                    st.dataframe(results['threshold_df'], use_container_width=True)

                with tab2:
                    st.subheader("RSEIç»¼åˆåˆ†æå¯è§†åŒ–")
                    st.image(results['img_path'], use_column_width=True)

                with tab3:
                    st.subheader("ä¸‹è½½ç»“æœæ–‡ä»¶")
                    col1, col2 = st.columns(2)

                    with col1:
                        with open(results['img_path'], 'rb') as f:
                            st.download_button(
                                "ğŸ“· ä¸‹è½½å¯è§†åŒ–å›¾ (PNG)",
                                f,
                                "RSEI_comprehensive.png",
                                "image/png",
                                use_container_width=True
                            )

                    with col2:
                        with open(results['excel_path'], 'rb') as f:
                            st.download_button(
                                "ğŸ“Š ä¸‹è½½ç»Ÿè®¡æŠ¥å‘Š (Excel)",
                                f,
                                "RSEI_analysis.xlsx",
                                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                use_container_width=True
                            )

                    st.markdown("---")

                    if export_geotiff:
                        st.subheader("ğŸ“¦ æ‰“åŒ…ä¸‹è½½")
                        st.info(f"å°†æ‰“åŒ… {len(results['saved_files'])} ä¸ªæ–‡ä»¶")

                        with st.expander("æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨"):
                            for file in results['saved_files']:
                                st.text(f"âœ“ {file}")

                        zip_buffer = io.BytesIO()
                        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                            output_path = results['output_path']
                            for file in output_path.glob('*'):
                                zip_file.write(file, file.name)

                            readme_content = f"""
RSEIè®¡ç®—ç»“æœè¯´æ˜
=================

è®¡ç®—æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
å«æ˜Ÿç±»å‹: {config.satellite}
åˆ†ç±»æ–¹æ³•: {'Jenksè‡ªç„¶é—´æ–­ç‚¹' if config.use_jenks else 'æ‰‹åŠ¨è®¾ç½®'}

ä¸»è¦æ–‡ä»¶:
---------
1. RSEI.tif - RSEIè¿ç»­å€¼ (0-1)
2. RSEI_classified.tif - RSEIåˆ†ç±» (1-5)
3. RSEI_comprehensive.png - ç»¼åˆå¯è§†åŒ–å›¾
4. RSEI_analysis.xlsx - ç»Ÿè®¡æŠ¥å‘Š

é¥æ„ŸæŒ‡æ•°:
---------
5. NDVI.tif - å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•°
6. WET.tif - æ¹¿åº¦æŒ‡æ•°
7. NDBSI.tif - å½’ä¸€åŒ–å»ºç­‘-åœŸå£¤æŒ‡æ•°
8. LST.tif - åœ°è¡¨æ¸©åº¦
9. NDBI.tif - å½’ä¸€åŒ–å»ºç­‘æŒ‡æ•°
10. SI.tif - åœŸå£¤æŒ‡æ•°

å‚è€ƒæ–‡çŒ®:
---------
Xu, H., et al. (2013). A remote sensing urban ecological index and its application.
Acta Ecologica Sinica, 33(24), 7853-7862.
"""
                            zip_file.writestr('README.txt', readme_content.encode('utf-8'))

                        zip_size = len(zip_buffer.getvalue()) / (1024 * 1024)

                        st.download_button(
                            f"ğŸ“¦ ä¸‹è½½æ‰€æœ‰ç»“æœ (ZIP) - {zip_size:.2f} MB",
                            zip_buffer.getvalue(),
                            f"RSEI_results_{time.strftime('%Y%m%d_%H%M%S')}.zip",
                            "application/zip",
                            use_container_width=True
                        )

                        st.success("âœ… ZIPåŒ…åŒ…å«æ‰€æœ‰è®¡ç®—ç»“æœå’Œé¥æ„ŸæŒ‡æ•°ï¼")

                with tab4:
                    st.subheader("è¾“å‡ºæ–‡ä»¶æ¸…å•")
                    st.dataframe(results['files_df'], use_container_width=True)
                    st.info(f"å…±ç”Ÿæˆ {len(results['saved_files'])} ä¸ªæ–‡ä»¶")

                with tab5:
                    st.subheader("è®¡ç®—è¯¦æƒ…")
                    col1, col2 = st.columns(2)

                    with col1:
                        st.metric("å«æ˜Ÿç±»å‹", config.satellite)
                        st.metric("è®¡ç®—æ–¹æ³•", 'PCA' if config.use_pca else 'ç›´æ¥è®¡ç®—')
                        st.metric("åˆ†ç±»æ–¹æ³•", 'Jenksè‡ªç„¶é—´æ–­ç‚¹' if config.use_jenks else 'æ‰‹åŠ¨è®¾ç½®')
                        st.metric("æ€»è€—æ—¶", f"{elapsed_time:.1f}ç§’")

                    with col2:
                        st.metric("æ°´ä½“æ©è†œ", 'æ˜¯' if config.mask_water else 'å¦')
                        if config.mask_water:
                            st.metric("æ°´ä½“æŒ‡æ•°", config.water_index)
                        st.metric("å¯¼å‡ºæ–‡ä»¶æ•°", len(results['saved_files']))

                    st.markdown("---")
                    st.write("**åˆ†ç±»é˜ˆå€¼:**", [f'{b:.4f}' for b in results['classification_breaks']])

    else:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ å¤šæ³¢æ®µTIFå½±åƒæ–‡ä»¶å¼€å§‹è®¡ç®—")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            ### ğŸŒŸ åŠŸèƒ½ç‰¹ç‚¹
            - âœ… æ”¯æŒ Landsat 8 å’Œ Sentinel-2
            - âœ… è‡ªåŠ¨æ°´ä½“æ©è†œï¼ˆOTSUï¼‰
            - âœ… Jenksè‡ªç„¶é—´æ–­ç‚¹åˆ†ç±»
            - âœ… **ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜**
            - âœ… å®Œæ•´å¯¼å‡ºæ‰€æœ‰æŒ‡æ•°
            - âœ… ä¸€é”®æ‰“åŒ…ä¸‹è½½

            ### ğŸ“‹ è¾“å…¥è¦æ±‚
            - **æ ¼å¼:** GeoTIFF (.tif/.tiff)
            - **æ³¢æ®µ:** æŒ‰é¡ºåºæ’åˆ—
            """)

        with col2:
            st.markdown("""
            ### ğŸ“Š è¾“å‡ºç»“æœ
            - ğŸ¯ RSEIè¿ç»­å€¼/åˆ†ç±»
            - ğŸ–¼ï¸ ç»¼åˆå¯è§†åŒ–å›¾
            - ğŸ“ˆ Excelç»Ÿè®¡æŠ¥å‘Š
            - ğŸŒ± 10+é¥æ„ŸæŒ‡æ•°

            ### ğŸ“š å‚è€ƒæ–‡çŒ®
            Xu, H., et al. (2013). RSEI.
            *Acta Ecologica Sinica*, 33(24).
            """)


if __name__ == "__main__":
    main()