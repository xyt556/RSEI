import streamlit as st
from pathlib import Path
import numpy as np
import rasterio
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from matplotlib.patches import Patch
from matplotlib import font_manager
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd
import warnings
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass, field
import time
import io
import zipfile
import tempfile
import shutil
import platform
import os
import urllib.request

warnings.filterwarnings('ignore')


# =============================
# å½»åº•è§£å†³ä¸­æ–‡å­—ä½“é—®é¢˜
# =============================
def download_chinese_font():
    """
    ä¸‹è½½å¼€æºä¸­æ–‡å­—ä½“ï¼ˆæ€æºé»‘ä½“ï¼‰
    """
    font_dir = Path.home() / '.fonts'
    font_dir.mkdir(exist_ok=True)
    font_path = font_dir / 'SourceHanSansSC-Regular.otf'

    if not font_path.exists():
        try:
            st.info("é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½ä¸­æ–‡å­—ä½“...")
            # ä½¿ç”¨GitHubé•œåƒä¸‹è½½æ€æºé»‘ä½“
            font_url = "https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Regular.otf"
            urllib.request.urlretrieve(font_url, font_path)
            st.success("å­—ä½“ä¸‹è½½å®Œæˆï¼")
            return font_path
        except Exception as e:
            st.warning(f"å­—ä½“ä¸‹è½½å¤±è´¥: {e}")
            return None
    return font_path


def setup_chinese_font_enhanced():
    """
    å¢å¼ºç‰ˆä¸­æ–‡å­—ä½“é…ç½® - å¤šé‡æ–¹æ¡ˆç¡®ä¿æˆåŠŸ
    """
    # æ–¹æ¡ˆ1: å°è¯•ä¸‹è½½å’Œä½¿ç”¨æ€æºé»‘ä½“
    downloaded_font = download_chinese_font()

    # æ¸…é™¤matplotlibå­—ä½“ç¼“å­˜
    try:
        import shutil
        cache_dir = matplotlib.get_cachedir()
        if os.path.exists(cache_dir):
            font_cache = os.path.join(cache_dir, 'fontlist-v330.json')
            if os.path.exists(font_cache):
                os.remove(font_cache)
    except Exception as e:
        pass

    # é‡æ–°åŠ è½½å­—ä½“
    font_manager._load_fontmanager(try_read_cache=False)

    # è·å–ç³»ç»Ÿç±»å‹
    system = platform.system()

    # æ ¹æ®ä¸åŒæ“ä½œç³»ç»Ÿè®¾ç½®å­—ä½“ä¼˜å…ˆçº§
    if system == 'Windows':
        font_list = ['Microsoft YaHei', 'SimHei', 'SimSun', 'KaiTi', 'FangSong', 'Microsoft YaHei UI']
    elif system == 'Darwin':  # macOS
        font_list = ['PingFang SC', 'Heiti SC', 'STHeiti', 'Arial Unicode MS', 'Songti SC']
    else:  # Linux
        font_list = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 'Noto Sans CJK SC',
                     'Noto Sans CJK JP', 'Droid Sans Fallback', 'AR PL UMing CN', 'Source Han Sans SC']

    # å¦‚æœä¸‹è½½äº†å­—ä½“ï¼Œæ·»åŠ åˆ°åˆ—è¡¨é¦–ä½
    if downloaded_font and downloaded_font.exists():
        font_list.insert(0, str(downloaded_font))

    # æ£€æµ‹ç³»ç»Ÿä¸­å¯ç”¨çš„å­—ä½“
    available_fonts = set(f.name for f in font_manager.fontManager.ttflist)

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    chinese_font = None
    font_path = None

    for font in font_list:
        # å¦‚æœæ˜¯è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        if os.path.exists(str(font)):
            font_path = font
            chinese_font = font
            break
        # å¦‚æœæ˜¯å­—ä½“åç§°ï¼Œæ£€æŸ¥æ˜¯å¦å¯ç”¨
        elif font in available_fonts:
            chinese_font = font
            break

    # é…ç½®matplotlib
    if chinese_font:
        if font_path:
            # å¦‚æœæ˜¯å­—ä½“æ–‡ä»¶è·¯å¾„ï¼Œæ·»åŠ åˆ°matplotlib
            font_manager.fontManager.addfont(str(font_path))
            font_prop = font_manager.FontProperties(fname=str(font_path))
            plt.rcParams['font.family'] = font_prop.get_name()
            plt.rcParams['font.sans-serif'] = [font_prop.get_name(), 'DejaVu Sans']
        else:
            plt.rcParams['font.sans-serif'] = [chinese_font, 'DejaVu Sans']

        plt.rcParams['axes.unicode_minus'] = False

        # å¼ºåˆ¶è®¾ç½®æ‰€æœ‰æ–‡æœ¬ç›¸å…³å‚æ•°
        plt.rcParams['font.size'] = 10
        plt.rcParams['axes.titlesize'] = 12
        plt.rcParams['axes.labelsize'] = 10
        plt.rcParams['xtick.labelsize'] = 9
        plt.rcParams['ytick.labelsize'] = 9
        plt.rcParams['legend.fontsize'] = 9

        return chinese_font
    else:
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        plt.rcParams['axes.unicode_minus'] = False

        # å°è¯•ä»æ‰€æœ‰å¯ç”¨å­—ä½“ä¸­æ‰¾CJKå­—ä½“
        cjk_fonts = [f.name for f in font_manager.fontManager.ttflist
                     if any(keyword in f.name.lower() for keyword in
                            ['cjk', 'chinese', 'sc', 'cn', 'hei', 'song', 'kai',
                             'pingfang', 'microsoft', 'simhei', 'wenquanyi', 'noto', 'source'])]

        if cjk_fonts:
            plt.rcParams['font.sans-serif'] = [cjk_fonts[0], 'DejaVu Sans']
            return cjk_fonts[0]
        else:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
            st.warning("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹æ¡†")
            return 'DejaVu Sans (æ— ä¸­æ–‡æ”¯æŒ)'


# åœ¨ç¨‹åºå¼€å§‹æ—¶è®¾ç½®å­—ä½“
try:
    detected_font = setup_chinese_font_enhanced()
    print(f"âœ“ ä½¿ç”¨å­—ä½“: {detected_font}")
except Exception as e:
    print(f"âœ— å­—ä½“è®¾ç½®å¤±è´¥: {e}")
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="RSEIè®¡ç®—ç³»ç»Ÿ",
    page_icon="ğŸŒ¿",
    layout="wide",
    initial_sidebar_state="expanded"
)


# =============================
# æ ¸å¿ƒè®¡ç®—ç±»ï¼ˆä¿æŒä¸å˜ï¼‰
# =============================
class JenksNaturalBreaks:
    """ä¼˜åŒ–çš„Jenksè‡ªç„¶é—´æ–­ç‚¹åˆ†ç±»ç®—æ³•"""

    @staticmethod
    def calculate_jenks_breaks(data: np.ndarray, n_classes: int = 5,
                               max_samples: int = 5000) -> List[float]:
        valid_data = data[~np.isnan(data)].flatten()
        if len(valid_data) == 0:
            raise ValueError("æ•°æ®å…¨ä¸ºNaN")

        if len(valid_data) > max_samples:
            st.info(f"æ•°æ®é‡ {len(valid_data):,} è¿‡å¤§ï¼Œé‡‡æ ·è‡³ {max_samples:,} ä¸ªç‚¹...")
            valid_data = JenksNaturalBreaks._stratified_sample(valid_data, max_samples)

        methods = [
            ('jenkspy', JenksNaturalBreaks._jenks_by_jenkspy),
            ('numba', JenksNaturalBreaks._jenks_by_numba),
            ('optimized_numpy', JenksNaturalBreaks._jenks_optimized_numpy),
            ('fallback', JenksNaturalBreaks._jenks_fallback)
        ]

        for method_name, method_func in methods:
            try:
                breaks = method_func(valid_data, n_classes)
                if breaks is not None:
                    st.success(f"ä½¿ç”¨æ–¹æ³•: {method_name}")
                    return breaks
            except Exception as e:
                if method_name != 'fallback':
                    st.warning(f"{method_name} æ–¹æ³•å¤±è´¥: {e}")
                    continue
                else:
                    raise

        return JenksNaturalBreaks.get_default_breaks()

    @staticmethod
    def _stratified_sample(data: np.ndarray, n_samples: int) -> np.ndarray:
        sorted_data = np.sort(data)
        indices = np.linspace(0, len(sorted_data) - 1, n_samples, dtype=int)
        np.random.seed(42)
        noise = np.random.randint(-2, 3, size=n_samples)
        indices = np.clip(indices + noise, 0, len(sorted_data) - 1)
        return sorted_data[indices]

    @staticmethod
    def _jenks_by_jenkspy(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        try:
            import jenkspy
            breaks = jenkspy.jenks_breaks(data, n_classes=n_classes)
            return breaks[1:-1]
        except ImportError:
            return None

    @staticmethod
    def _jenks_by_numba(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        try:
            from numba import jit

            @jit(nopython=True)
            def _jenks_matrices_numba(data, n_classes):
                n_data = len(data)
                mat1 = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)
                mat2 = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)

                for i in range(1, n_classes + 1):
                    mat1[1, i] = 1
                    mat2[1, i] = 0
                    for j in range(2, n_data + 1):
                        mat2[j, i] = np.inf

                for l in range(2, n_data + 1):
                    s1 = 0.0
                    s2 = 0.0

                    for m in range(1, l + 1):
                        i3 = l - m + 1
                        val = data[i3 - 1]
                        s2 += val * val
                        s1 += val
                        w = m
                        v = s2 - (s1 * s1) / w
                        i4 = i3 - 1

                        if i4 != 0:
                            for j in range(2, n_classes + 1):
                                if mat2[l, j] >= (v + mat2[i4, j - 1]):
                                    mat1[l, j] = i3
                                    mat2[l, j] = v + mat2[i4, j - 1]

                    mat1[l, 1] = 1
                    mat2[l, 1] = v

                return mat1, mat2

            sorted_data = np.sort(data)
            mat1, mat2 = _jenks_matrices_numba(sorted_data, n_classes)

            k = len(sorted_data)
            kclass = []

            for j in range(n_classes, 0, -1):
                idx = int(mat1[k, j]) - 2
                if idx >= 0 and idx < len(sorted_data):
                    kclass.append(sorted_data[idx])
                k = int(mat1[k, j]) - 1

            kclass.reverse()
            if len(kclass) > 1:
                return kclass[1:][:n_classes - 1]
            else:
                return None

        except ImportError:
            return None

    @staticmethod
    def _jenks_optimized_numpy(data: np.ndarray, n_classes: int) -> Optional[List[float]]:
        sorted_data = np.sort(data)
        n_data = len(sorted_data)

        lower_class_limits = np.zeros((n_data + 1, n_classes + 1), dtype=np.int32)
        variance_combinations = np.zeros((n_data + 1, n_classes + 1), dtype=np.float64)
        variance_combinations[:, :] = np.inf

        lower_class_limits[1:, 1] = 1
        variance_combinations[1, :] = 0

        for l in range(2, n_data + 1):
            sum_val = 0.0
            sum_sq = 0.0

            for m in range(1, l + 1):
                lower_class_limit = l - m + 1
                val = sorted_data[lower_class_limit - 1]

                sum_val += val
                sum_sq += val * val
                w = m

                variance = sum_sq - (sum_val * sum_val) / w

                if lower_class_limit > 1:
                    for j in range(2, n_classes + 1):
                        if variance_combinations[l, j] >= (
                                variance + variance_combinations[lower_class_limit - 1, j - 1]):
                            lower_class_limits[l, j] = lower_class_limit
                            variance_combinations[l, j] = variance + variance_combinations[
                                lower_class_limit - 1, j - 1]

            lower_class_limits[l, 1] = 1
            variance_combinations[l, 1] = variance

        k = n_data
        breaks = []

        for j in range(n_classes, 1, -1):
            idx = lower_class_limits[k, j] - 2
            if 0 <= idx < len(sorted_data):
                breaks.append(float(sorted_data[idx]))
            k = lower_class_limits[k, j] - 1

        breaks.reverse()

        if len(breaks) >= n_classes - 1:
            return breaks[:n_classes - 1]
        else:
            return None

    @staticmethod
    def _jenks_fallback(data: np.ndarray, n_classes: int) -> List[float]:
        sorted_data = np.sort(data)
        n = len(sorted_data)
        breaks = []

        quantiles = np.linspace(0, 1, n_classes + 1)[1:-1]

        for q in quantiles:
            idx = int(q * n)
            window = min(int(n * 0.05), 100)
            start = max(0, idx - window)
            end = min(n, idx + window)

            if start >= end - 1:
                breaks.append(sorted_data[idx])
                continue

            min_variance = np.inf
            best_idx = idx

            for i in range(start, end):
                if i == 0 or i == n:
                    continue

                left_var = np.var(sorted_data[max(0, i - window):i]) if i > window else 0
                right_var = np.var(sorted_data[i:min(n, i + window)]) if i < n - window else 0
                total_var = left_var + right_var

                if total_var < min_variance:
                    min_variance = total_var
                    best_idx = i

            breaks.append(float(sorted_data[best_idx]))

        return breaks[:n_classes - 1]

    @staticmethod
    def get_default_breaks() -> List[float]:
        return [0.2, 0.4, 0.6, 0.8]


@dataclass
class BandConfig:
    blue: int = 1
    green: int = 2
    red: int = 3
    nir: int = 4
    swir1: int = 5
    swir2: int = 6
    tir: int = 7

    @classmethod
    def landsat8_stacked(cls):
        return cls(blue=1, green=2, red=3, nir=4, swir1=5, swir2=6, tir=7)

    @classmethod
    def sentinel2_stacked(cls):
        return cls(blue=1, green=2, red=3, nir=4, swir1=5, swir2=6, tir=None)


@dataclass
class RSEIConfig:
    satellite: str = 'Landsat8'
    band_config: BandConfig = None
    use_pca: bool = True
    export_indices: bool = True
    export_geotiff: bool = True
    mask_water: bool = True
    water_index: str = 'MNDWI'
    water_threshold: Optional[float] = None
    use_otsu: bool = True
    use_jenks: bool = True
    classification_breaks: List[float] = field(default_factory=lambda: [0.2, 0.4, 0.6, 0.8])
    jenks_samples: int = 5000

    def __post_init__(self):
        if self.band_config is None:
            if self.satellite.lower() == 'landsat8':
                self.band_config = BandConfig.landsat8_stacked()
            elif self.satellite.lower() == 'sentinel2':
                self.band_config = BandConfig.sentinel2_stacked()
            else:
                self.band_config = BandConfig()


class MultiSpectralImageReader:
    def __init__(self, config: RSEIConfig):
        self.config = config
        self.metadata = None
        self.bands_data = {}

    def read_multiband_tif(self, tif_path: str) -> Dict[str, np.ndarray]:
        st.info(f"ğŸ“¡ è¯»å–å¤šæ³¢æ®µå½±åƒ: {Path(tif_path).name}")

        with rasterio.open(tif_path) as src:
            st.write(f"å°ºå¯¸: {src.width} x {src.height}, æ³¢æ®µæ•°: {src.count}")

            self.metadata = {
                'transform': src.transform,
                'crs': src.crs,
                'width': src.width,
                'height': src.height,
                'dtype': 'float32',
                'count': 1,
                'driver': 'GTiff',
                'nodata': np.nan
            }

            required_bands = self._get_required_bands()
            max_band_idx = max([idx for idx in required_bands.values() if idx is not None])

            if src.count < max_band_idx:
                raise ValueError(f"æ³¢æ®µæ•°é‡ä¸è¶³ï¼éœ€è¦{max_band_idx}ä¸ªï¼Œå®é™…{src.count}ä¸ª")

            bands = {}
            for band_name, band_idx in required_bands.items():
                if band_idx is None:
                    continue

                band_data = src.read(band_idx).astype(float)
                if src.nodata is not None:
                    band_data[band_data == src.nodata] = np.nan
                band_data[band_data < -9999] = np.nan
                band_data[band_data > 50000] = np.nan
                bands[band_name] = band_data

        st.success(f"âœ… æˆåŠŸè¯»å– {len(bands)} ä¸ªæ³¢æ®µ")
        self.bands_data = bands
        return bands

    def _get_required_bands(self) -> Dict[str, int]:
        bc = self.config.band_config
        return {
            'blue': bc.blue, 'green': bc.green, 'red': bc.red,
            'nir': bc.nir, 'swir1': bc.swir1, 'swir2': bc.swir2,
            'tir': bc.tir
        }

    def apply_scale_factor(self, bands: Dict[str, np.ndarray],
                           scale_factor: float = 0.0001) -> Dict[str, np.ndarray]:
        st.info(f"ğŸ”§ åº”ç”¨ç¼©æ”¾å› å­: {scale_factor}")
        scaled_bands = {}
        optical_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']

        for band_name, band_data in bands.items():
            if band_name in optical_bands:
                scaled_bands[band_name] = band_data * scale_factor
            else:
                scaled_bands[band_name] = band_data
        return scaled_bands


class OTSUThreshold:
    @staticmethod
    def calculate_otsu_threshold(data: np.ndarray, bins: int = 256) -> Tuple[float, Dict]:
        valid_data = data[~np.isnan(data)]
        if len(valid_data) == 0:
            raise ValueError("æ•°æ®å…¨ä¸ºNaN")

        hist, bin_edges = np.histogram(valid_data, bins=bins)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        hist = hist.astype(float)
        prob = hist / hist.sum()

        max_variance = 0
        optimal_threshold = 0

        w0 = np.cumsum(prob)
        w1 = 1 - w0
        mu = np.cumsum(prob * bin_centers)
        mu_total = mu[-1]

        for i in range(1, bins):
            if w0[i] == 0 or w1[i] == 0:
                continue

            mu0 = mu[i] / w0[i] if w0[i] > 0 else 0
            mu1 = (mu_total - mu[i]) / w1[i] if w1[i] > 0 else 0
            variance = w0[i] * w1[i] * (mu0 - mu1) ** 2

            if variance > max_variance:
                max_variance = variance
                optimal_threshold = bin_centers[i]

        threshold_idx = np.argmin(np.abs(bin_centers - optimal_threshold))

        metrics = {
            'threshold': optimal_threshold,
            'max_variance': max_variance,
            'histogram': hist,
            'bin_centers': bin_centers,
            'threshold_idx': threshold_idx,
            'background_prob': w0[threshold_idx],
            'foreground_prob': w1[threshold_idx]
        }

        return optimal_threshold, metrics


class WaterMaskGenerator:
    @staticmethod
    def calculate_mndwi(green: np.ndarray, swir1: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            mndwi = (green - swir1) / (green + swir1)
        return mndwi

    @staticmethod
    def calculate_ndwi(green: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndwi = (green - nir) / (green + nir)
        return ndwi

    @staticmethod
    def calculate_aweish(blue: np.ndarray, green: np.ndarray,
                         nir: np.ndarray, swir1: np.ndarray,
                         swir2: np.ndarray) -> np.ndarray:
        aweish = blue + 2.5 * green - 1.5 * (nir + swir1) - 0.25 * swir2
        return aweish

    @staticmethod
    def create_water_mask(bands: Dict[str, np.ndarray],
                          method: str = 'MNDWI',
                          threshold: Optional[float] = None,
                          use_otsu: bool = True) -> Tuple[np.ndarray, np.ndarray, float]:
        st.info(f"ğŸ’§ åˆ›å»ºæ°´ä½“æ©è†œ (æ–¹æ³•: {method})")

        if method.upper() == 'NDWI':
            water_index = WaterMaskGenerator.calculate_ndwi(bands['green'], bands['nir'])
        elif method.upper() == 'MNDWI':
            water_index = WaterMaskGenerator.calculate_mndwi(bands['green'], bands['swir1'])
        elif method.upper() == 'AWEISH':
            water_index = WaterMaskGenerator.calculate_aweish(
                bands['blue'], bands['green'], bands['nir'],
                bands['swir1'], bands['swir2']
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ°´ä½“æŒ‡æ•°: {method}")

        if use_otsu and threshold is None:
            try:
                otsu_threshold, metrics = OTSUThreshold.calculate_otsu_threshold(water_index, bins=256)
                final_threshold = otsu_threshold
                st.success(f"âœ“ OTSUé˜ˆå€¼: {final_threshold:.4f}")
            except Exception as e:
                st.warning(f"âš ï¸ OTSUå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é˜ˆå€¼0.0")
                final_threshold = 0.0
        elif threshold is not None:
            final_threshold = threshold
        else:
            final_threshold = 0.0

        water_mask = water_index > final_threshold

        total_pixels = np.sum(~np.isnan(water_index))
        water_pixels = np.sum(water_mask)
        water_ratio = water_pixels / total_pixels * 100 if total_pixels > 0 else 0

        st.write(f"æ°´åŸŸ: {water_pixels:,} ({water_ratio:.2f}%)")

        return water_index, water_mask, final_threshold


class RemoteSensingIndices:
    @staticmethod
    def calculate_ndvi(red: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndvi = (nir - red) / (nir + red)
        return np.clip(ndvi, -1, 1)

    @staticmethod
    def calculate_ndbi(swir1: np.ndarray, nir: np.ndarray) -> np.ndarray:
        with np.errstate(divide='ignore', invalid='ignore'):
            ndbi = (swir1 - nir) / (swir1 + nir)
        return ndbi

    @staticmethod
    def calculate_ndbsi(bands: Dict[str, np.ndarray]) -> np.ndarray:
        swir1, red, nir, blue = bands['swir1'], bands['red'], bands['nir'], bands['blue']
        with np.errstate(divide='ignore', invalid='ignore'):
            si = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue))
            ndbi = RemoteSensingIndices.calculate_ndbi(swir1, nir)
            ndbsi = (si + ndbi) / 2
        return ndbsi

    @staticmethod
    def calculate_wet(bands: Dict[str, np.ndarray], sensor: str = 'Landsat8') -> np.ndarray:
        coeffs = {
            'blue': 0.1511, 'green': 0.1973, 'red': 0.3283,
            'nir': 0.3407, 'swir1': -0.7117, 'swir2': -0.4559
        }
        wet = np.zeros_like(bands['blue'])
        for band_name, coeff in coeffs.items():
            if band_name in bands:
                wet += coeff * bands[band_name]
        return wet

    @staticmethod
    def calculate_lst_simple(tir: np.ndarray) -> np.ndarray:
        if np.nanmax(tir) > 400:
            ML = 3.3420E-04
            AL = 0.10000
            K1 = 774.8853
            K2 = 1321.0789
            L_lambda = ML * tir + AL
            with np.errstate(divide='ignore', invalid='ignore'):
                T_b = K2 / np.log(K1 / L_lambda + 1)
        else:
            T_b = tir
        return T_b - 273.15


class RSEICalculator:
    def __init__(self, config: RSEIConfig = None):
        self.config = config or RSEIConfig()
        self.indices = {}
        self.rsei = None
        self.rsei_components = None
        self.calculated_breaks = None
        self.jenks_time = 0

    def normalize(self, array: np.ndarray, inverse: bool = False) -> np.ndarray:
        valid_mask = ~np.isnan(array)
        if not valid_mask.any():
            return array

        min_val = np.nanmin(array)
        max_val = np.nanmax(array)

        if max_val == min_val:
            return np.ones_like(array) * 0.5

        normalized = (array - min_val) / (max_val - min_val)
        if inverse:
            normalized = 1 - normalized
        return normalized

    def apply_water_mask(self, array: np.ndarray, water_mask: np.ndarray) -> np.ndarray:
        masked_array = array.copy()
        masked_array[water_mask] = np.nan
        return masked_array

    def calculate_rsei_pca(self, greenness: np.ndarray, wetness: np.ndarray,
                           dryness: np.ndarray, heat: np.ndarray,
                           water_mask: Optional[np.ndarray] = None) -> np.ndarray:
        st.info("ğŸ”¬ è®¡ç®—RSEI (PCAæ–¹æ³•)...")

        if water_mask is not None:
            greenness = self.apply_water_mask(greenness, water_mask)
            wetness = self.apply_water_mask(wetness, water_mask)
            dryness = self.apply_water_mask(dryness, water_mask)
            heat = self.apply_water_mask(heat, water_mask)

        green_norm = self.normalize(greenness, inverse=False)
        wet_norm = self.normalize(wetness, inverse=False)
        dry_norm = self.normalize(dryness, inverse=True)
        heat_norm = self.normalize(heat, inverse=True)

        mask = ~(np.isnan(green_norm) | np.isnan(wet_norm) |
                 np.isnan(dry_norm) | np.isnan(heat_norm))

        n_valid = mask.sum()
        st.write(f"æœ‰æ•ˆåƒç´ : {n_valid:,}")

        if n_valid < 100:
            raise ValueError("æœ‰æ•ˆåƒç´ å¤ªå°‘")

        data_matrix = np.column_stack([
            green_norm[mask], wet_norm[mask],
            dry_norm[mask], heat_norm[mask]
        ])

        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data_matrix)

        pca = PCA(n_components=4)
        pc_all = pca.fit_transform(data_scaled)
        pc1 = pc_all[:, 0]

        rsei_raw = np.full(greenness.shape, np.nan)
        rsei_raw[mask] = pc1.flatten()
        rsei = self.normalize(rsei_raw, inverse=False)

        st.success(f"PC1è´¡çŒ®ç‡: {pca.explained_variance_ratio_[0] * 100:.2f}%")

        self.rsei_components = {
            'greenness': green_norm,
            'wetness': wet_norm,
            'dryness': dry_norm,
            'heat': heat_norm,
            'pca': {
                'variance_ratio': pca.explained_variance_ratio_.tolist(),
                'components': pca.components_.tolist()
            }
        }

        if self.config.use_jenks:
            st.info("ğŸ” è®¡ç®—Jenksè‡ªç„¶é—´æ–­ç‚¹é˜ˆå€¼...")
            start_time = time.time()
            try:
                breaks = JenksNaturalBreaks.calculate_jenks_breaks(
                    rsei,
                    n_classes=5,
                    max_samples=self.config.jenks_samples
                )
                self.calculated_breaks = breaks
                self.jenks_time = time.time() - start_time
                st.success(f"âœ“ Jenksé˜ˆå€¼: {[f'{b:.4f}' for b in breaks]}")
                st.success(f"âœ“ è®¡ç®—è€—æ—¶: {self.jenks_time:.2f}ç§’")
            except Exception as e:
                st.warning(f"âš ï¸ Jenksè®¡ç®—å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é˜ˆå€¼")
                self.calculated_breaks = JenksNaturalBreaks.get_default_breaks()
                self.jenks_time = 0
        else:
            self.calculated_breaks = self.config.classification_breaks

        return rsei

    def classify_rsei(self, rsei: np.ndarray, breaks: Optional[List[float]] = None) -> np.ndarray:
        if breaks is None:
            breaks = self.calculated_breaks or self.config.classification_breaks

        classified = np.full_like(rsei, np.nan)

        if len(breaks) < 4:
            breaks = JenksNaturalBreaks.get_default_breaks()

        t1, t2, t3, t4 = breaks[0], breaks[1], breaks[2], breaks[3]

        classified[rsei < t1] = 1
        classified[(rsei >= t1) & (rsei < t2)] = 2
        classified[(rsei >= t2) & (rsei < t3)] = 3
        classified[(rsei >= t3) & (rsei < t4)] = 4
        classified[rsei >= t4] = 5

        return classified


class RSEIVisualizer:
    @staticmethod
    def create_comprehensive_visualization(rsei, rsei_class, indices,
                                           water_index=None, water_threshold=None,
                                           classification_breaks=None):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾ - å½»åº•ä¿®å¤ä¸­æ–‡æ˜¾ç¤º"""

        # é‡æ–°ç¡®è®¤å­—ä½“è®¾ç½®ï¼ˆåœ¨ç»˜å›¾å‰ï¼‰
        try:
            setup_chinese_font_enhanced()
        except:
            pass

        rsei_colors = ['#d73027', '#fc8d59', '#fee08b', '#91cf60', '#1a9850']
        rsei_cmap = ListedColormap(rsei_colors)

        # åˆ›å»ºå›¾å½¢
        fig = plt.figure(figsize=(22, 13))
        gs = fig.add_gridspec(3, 4, hspace=0.35, wspace=0.35)

        # 1. RSEIè¿ç»­å€¼
        ax1 = fig.add_subplot(gs[0, 0])
        im1 = ax1.imshow(rsei, cmap='RdYlGn', vmin=0, vmax=1)
        ax1.set_title('RSEI (è¿ç»­å€¼)', fontsize=14, fontweight='bold', pad=10)
        ax1.axis('off')
        cbar1 = plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)
        cbar1.ax.tick_params(labelsize=10)

        # 2. RSEIåˆ†ç±»
        ax2 = fig.add_subplot(gs[0, 1])
        im2 = ax2.imshow(rsei_class, cmap=rsei_cmap, vmin=1, vmax=5)

        if classification_breaks:
            breaks_str = f"[{classification_breaks[0]:.2f}, {classification_breaks[1]:.2f}, " \
                         f"{classification_breaks[2]:.2f}, {classification_breaks[3]:.2f}]"
            ax2.set_title(f'RSEIåˆ†ç±»\né˜ˆå€¼: {breaks_str}', fontsize=12, fontweight='bold', pad=10)
        else:
            ax2.set_title('RSEIåˆ†ç±»', fontsize=14, fontweight='bold', pad=10)
        ax2.axis('off')

        class_names = ['å·®', 'è¾ƒå·®', 'ä¸­ç­‰', 'è‰¯å¥½', 'ä¼˜ç§€']
        legend_elements = [Patch(facecolor=rsei_colors[i], label=class_names[i])
                           for i in range(5)]
        ax2.legend(handles=legend_elements, loc='upper right', fontsize=11)

        # 3-6. å››ä¸ªæŒ‡æ•°
        indices_to_plot = [
            ('ndvi', 'NDVI (ç»¿åº¦)', 'Greens'),
            ('wet', 'WET (æ¹¿åº¦)', 'Blues'),
            ('ndbsi', 'NDBSI (å¹²åº¦)', 'YlOrRd'),
            ('lst', 'LST (çƒ­åº¦)', 'hot')
        ]

        for idx, (key, title, cmap) in enumerate(indices_to_plot):
            ax = fig.add_subplot(gs[0, 2 + idx % 2] if idx < 2 else gs[1, idx - 2])
            if key in indices:
                im = ax.imshow(indices[key], cmap=cmap)
                ax.set_title(title, fontsize=14, fontweight='bold', pad=10)
                ax.axis('off')
                cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
                cbar.ax.tick_params(labelsize=10)

        # 7. æ°´ä½“æ©è†œ
        if water_index is not None:
            ax7 = fig.add_subplot(gs[1, 2])
            im7 = ax7.imshow(water_index, cmap='RdYlBu')
            ax7.set_title(f'æ°´ä½“æŒ‡æ•° (é˜ˆå€¼={water_threshold:.3f})', fontsize=14, fontweight='bold', pad=10)
            ax7.axis('off')
            cbar7 = plt.colorbar(im7, ax=ax7, fraction=0.046, pad=0.04)
            cbar7.ax.tick_params(labelsize=10)

        # 8. RSEIç»Ÿè®¡ç›´æ–¹å›¾
        ax8 = fig.add_subplot(gs[1, 3])
        valid_rsei = rsei[~np.isnan(rsei)]
        ax8.hist(valid_rsei, bins=50, color='steelblue', edgecolor='black', alpha=0.7)
        ax8.axvline(np.nanmean(rsei), color='red', linestyle='--',
                    label=f'å‡å€¼={np.nanmean(rsei):.3f}', linewidth=2.5)

        if classification_breaks:
            colors_line = ['#d73027', '#fc8d59', '#fee08b', '#91cf60']
            for i, threshold in enumerate(classification_breaks):
                ax8.axvline(threshold, color=colors_line[i], linestyle=':',
                            linewidth=2, alpha=0.8)

        ax8.set_title('RSEIåˆ†å¸ƒ', fontsize=14, fontweight='bold', pad=10)
        ax8.set_xlabel('RSEIå€¼', fontsize=12)
        ax8.set_ylabel('é¢‘æ•°', fontsize=12)
        ax8.legend(fontsize=11)
        ax8.grid(alpha=0.3)
        ax8.tick_params(labelsize=10)

        # 9. ç­‰çº§é¢ç§¯ç»Ÿè®¡
        ax9 = fig.add_subplot(gs[2, :2])
        class_counts = [np.sum(rsei_class == i) for i in range(1, 6)]
        colors = rsei_colors
        bars = ax9.bar(class_names, class_counts, color=colors, edgecolor='black', alpha=0.85, linewidth=1.5)
        ax9.set_title('RSEIç­‰çº§é¢ç§¯ç»Ÿè®¡', fontsize=14, fontweight='bold', pad=10)
        ax9.set_ylabel('åƒç´ æ•°é‡', fontsize=12)
        ax9.grid(axis='y', alpha=0.3)
        ax9.tick_params(labelsize=11)

        for bar, count in zip(bars, class_counts):
            height = bar.get_height()
            ax9.text(bar.get_x() + bar.get_width() / 2., height,
                     f'{int(count):,}\n({count / np.sum(class_counts) * 100:.1f}%)',
                     ha='center', va='bottom', fontsize=11, fontweight='bold')

        # 10. ç­‰çº§é¢ç§¯é¥¼å›¾
        ax10 = fig.add_subplot(gs[2, 2:])
        wedges, texts, autotexts = ax10.pie(class_counts, labels=class_names,
                                            colors=colors, autopct='%1.1f%%',
                                            startangle=90, textprops={'fontsize': 12})
        ax10.set_title('RSEIç­‰çº§æ¯”ä¾‹', fontsize=14, fontweight='bold', pad=10)

        for text in texts:
            text.set_fontsize(12)
            text.set_fontweight('bold')
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
            autotext.set_fontsize(12)

        title_text = 'RSEIç»¼åˆåˆ†æç»“æœ'
        if classification_breaks:
            method = "Jenksè‡ªç„¶é—´æ–­ç‚¹" if classification_breaks != [0.2, 0.4, 0.6, 0.8] else "ç­‰é—´è·"
            title_text += f' ({method}åˆ†ç±»)'
        fig.suptitle(title_text, fontsize=18, fontweight='bold', y=0.98)

        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout(rect=[0, 0, 1, 0.97])

        return fig


# ç”±äºä»£ç å¤ªé•¿ï¼Œexecute_rsei_calculationå’Œmainå‡½æ•°ä¿æŒä¸ä¹‹å‰ç›¸åŒ
# åªéœ€ç¡®ä¿åœ¨è°ƒç”¨RSEIVisualizeræ—¶å­—ä½“å·²æ­£ç¡®é…ç½®

def execute_rsei_calculation(input_file, config):
    """æ ¸å¿ƒè®¡ç®—é€»è¾‘"""
    temp_dir = tempfile.mkdtemp()
    output_path = Path(temp_dir)

    try:
        progress_bar = st.progress(0)
        status_text = st.empty()

        # è¯»å–å’Œå¤„ç†ï¼ˆä¿æŒä»£ç ä¸å˜ï¼‰
        status_text.text("æ­¥éª¤1/9: è¯»å–å½±åƒ...")
        progress_bar.progress(10)
        reader = MultiSpectralImageReader(config)
        bands = reader.read_multiband_tif(input_file)

        status_text.text("æ­¥éª¤2/9: æ•°æ®é¢„å¤„ç†...")
        progress_bar.progress(15)
        max_val = np.nanmax(bands['red'])
        if max_val > 1.0:
            bands = reader.apply_scale_factor(bands, 0.0001)

        water_index = None
        water_mask = None
        water_threshold_used = None

        if config.mask_water:
            status_text.text("æ­¥éª¤3/9: åˆ›å»ºæ°´ä½“æ©è†œ...")
            progress_bar.progress(25)
            water_index, water_mask, water_threshold_used = WaterMaskGenerator.create_water_mask(
                bands, config.water_index, config.water_threshold, config.use_otsu
            )

        status_text.text("æ­¥éª¤4/9: è®¡ç®—é¥æ„ŸæŒ‡æ•°...")
        progress_bar.progress(35)
        calc = RemoteSensingIndices()
        ndvi = calc.calculate_ndvi(bands['red'], bands['nir'])
        wet = calc.calculate_wet(bands, config.satellite)
        ndbsi = calc.calculate_ndbsi(bands)

        ndbi = calc.calculate_ndbi(bands['swir1'], bands['nir'])
        swir1, red, nir, blue = bands['swir1'], bands['red'], bands['nir'], bands['blue']
        with np.errstate(divide='ignore', invalid='ignore'):
            si = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue))

        if 'tir' in bands and bands['tir'] is not None:
            lst = calc.calculate_lst_simple(bands['tir'])
        else:
            lst = ndbsi

        indices = {
            'ndvi': ndvi,
            'wet': wet,
            'ndbsi': ndbsi,
            'lst': lst,
            'ndbi': ndbi,
            'si': si
        }

        status_text.text("æ­¥éª¤5/9: è®¡ç®—RSEI...")
        progress_bar.progress(45)
        rsei_calc = RSEICalculator(config)
        rsei = rsei_calc.calculate_rsei_pca(ndvi, wet, ndbsi, lst, water_mask)

        status_text.text("æ­¥éª¤6/9: RSEIåˆ†ç±»...")
        progress_bar.progress(55)
        classification_breaks = rsei_calc.calculated_breaks
        rsei_class = rsei_calc.classify_rsei(rsei, classification_breaks)

        class_names = ['å·®', 'è¾ƒå·®', 'ä¸­ç­‰', 'è‰¯å¥½', 'ä¼˜ç§€']
        total_valid = np.sum(~np.isnan(rsei_class))

        st.write(f"\nä½¿ç”¨çš„åˆ†ç±»é˜ˆå€¼: {[f'{b:.4f}' for b in classification_breaks]}")
        st.write("\nç­‰çº§åˆ†å¸ƒ:")
        for i, name in enumerate(class_names, 1):
            count = np.sum(rsei_class == i)
            ratio = count / total_valid * 100 if total_valid > 0 else 0
            st.write(f"{name}: {count:,} ({ratio:.2f}%)")

        status_text.text("æ­¥éª¤7/9: ç”Ÿæˆå¯è§†åŒ–å›¾...")
        progress_bar.progress(65)

        # åœ¨ç»˜å›¾å‰å†æ¬¡ç¡®è®¤å­—ä½“
        try:
            setup_chinese_font_enhanced()
        except:
            pass

        fig = RSEIVisualizer.create_comprehensive_visualization(
            rsei, rsei_class, indices, water_index,
            water_threshold_used, classification_breaks
        )

        img_path = output_path / 'RSEI_comprehensive.png'
        # ä¿å­˜æ—¶ä½¿ç”¨æ›´é«˜çš„DPIå’Œç¡®ä¿å­—ä½“åµŒå…¥
        fig.savefig(img_path, dpi=300, bbox_inches='tight',
                    facecolor='white', edgecolor='none')
        plt.close(fig)

        # å¯¼å‡ºæ–‡ä»¶éƒ¨åˆ†ä¿æŒä¸å˜...
        status_text.text("æ­¥éª¤8/9: å¯¼å‡ºGeoTIFFæ–‡ä»¶...")
        progress_bar.progress(75)

        saved_files = []

        if config.export_geotiff and reader.metadata:
            with rasterio.open(output_path / 'RSEI.tif', 'w', **reader.metadata) as dst:
                dst.write(rsei.astype('float32'), 1)
            saved_files.append('RSEI.tif')

            with rasterio.open(output_path / 'RSEI_classified.tif', 'w', **reader.metadata) as dst:
                dst.write(rsei_class.astype('float32'), 1)
            saved_files.append('RSEI_classified.tif')

            if water_index is not None:
                with rasterio.open(output_path / 'Water_Index.tif', 'w', **reader.metadata) as dst:
                    dst.write(water_index.astype('float32'), 1)
                saved_files.append('Water_Index.tif')

                with rasterio.open(output_path / 'Water_Mask.tif', 'w', **reader.metadata) as dst:
                    dst.write(water_mask.astype('float32'), 1)
                saved_files.append('Water_Mask.tif')

            if config.export_indices:
                st.info("æ­£åœ¨å¯¼å‡ºæ‰€æœ‰é¥æ„ŸæŒ‡æ•°...")

                index_files = {
                    'NDVI.tif': ndvi,
                    'WET.tif': wet,
                    'NDBSI.tif': ndbsi,
                    'LST.tif': lst,
                    'NDBI.tif': ndbi,
                    'SI.tif': si,
                    'Greenness_Normalized.tif': rsei_calc.rsei_components['greenness'],
                    'Wetness_Normalized.tif': rsei_calc.rsei_components['wetness'],
                    'Dryness_Normalized.tif': rsei_calc.rsei_components['dryness'],
                    'Heat_Normalized.tif': rsei_calc.rsei_components['heat']
                }

                for filename, data in index_files.items():
                    with rasterio.open(output_path / filename, 'w', **reader.metadata) as dst:
                        dst.write(data.astype('float32'), 1)
                    saved_files.append(filename)

        # Excelç»Ÿè®¡
        status_text.text("æ­¥éª¤9/9: ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        progress_bar.progress(85)

        stats_df = pd.DataFrame({
            'æŒ‡æ ‡': ['NDVI', 'WET', 'NDBSI', 'LST', 'NDBI', 'SI', 'RSEI'],
            'æœ€å°å€¼': [f"{np.nanmin(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'æœ€å¤§å€¼': [f"{np.nanmax(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'å‡å€¼': [f"{np.nanmean(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]],
            'æ ‡å‡†å·®': [f"{np.nanstd(x):.4f}" for x in [ndvi, wet, ndbsi, lst, ndbi, si, rsei]]
        })

        class_df = pd.DataFrame({
            'ç­‰çº§': class_names,
            'åƒç´ æ•°': [int(np.sum(rsei_class == i)) for i in range(1, 6)],
            'ç™¾åˆ†æ¯”': [f"{np.sum(rsei_class == i) / total_valid * 100:.2f}%" for i in range(1, 6)]
        })

        threshold_df = pd.DataFrame({
            'åˆ†ç±»é˜ˆå€¼': ['å·®/è¾ƒå·®', 'è¾ƒå·®/ä¸­ç­‰', 'ä¸­ç­‰/è‰¯å¥½', 'è‰¯å¥½/ä¼˜ç§€'],
            'é˜ˆå€¼': [f"{b:.4f}" for b in classification_breaks],
            'æ–¹æ³•': ['Jenksè‡ªç„¶é—´æ–­ç‚¹' if config.use_jenks else 'æ‰‹åŠ¨è®¾ç½®'] * 4,
            'Jenksè€—æ—¶(ç§’)': [f"{rsei_calc.jenks_time:.2f}" if config.use_jenks else 'N/A'] * 4
        })

        files_df = pd.DataFrame({
            'æ–‡ä»¶å': saved_files,
            'è¯´æ˜': [
                'RSEIè¿ç»­å€¼ï¼ˆ0-1ï¼‰',
                'RSEIåˆ†ç±»ï¼ˆ1-5ï¼‰',
                'æ°´ä½“æŒ‡æ•°',
                'æ°´ä½“æ©è†œ',
                'å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•°',
                'æ¹¿åº¦æŒ‡æ•°',
                'å½’ä¸€åŒ–å»ºç­‘-åœŸå£¤æŒ‡æ•°',
                'åœ°è¡¨æ¸©åº¦',
                'å½’ä¸€åŒ–å»ºç­‘æŒ‡æ•°',
                'åœŸå£¤æŒ‡æ•°',
                'ç»¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'æ¹¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'å¹²åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰',
                'çƒ­åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰'
            ][:len(saved_files)]
        })

        excel_path = output_path / 'RSEI_analysis.xlsx'
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            stats_df.to_excel(writer, sheet_name='æŒ‡æ ‡ç»Ÿè®¡', index=False)
            class_df.to_excel(writer, sheet_name='ç­‰çº§åˆ†å¸ƒ', index=False)
            threshold_df.to_excel(writer, sheet_name='åˆ†ç±»é˜ˆå€¼', index=False)
            files_df.to_excel(writer, sheet_name='æ–‡ä»¶æ¸…å•', index=False)

        progress_bar.progress(100)
        status_text.text("âœ… è®¡ç®—å®Œæˆï¼")

        return {
            'rsei': rsei,
            'rsei_class': rsei_class,
            'indices': indices,
            'stats_df': stats_df,
            'class_df': class_df,
            'threshold_df': threshold_df,
            'files_df': files_df,
            'img_path': str(img_path),
            'excel_path': str(excel_path),
            'output_path': output_path,
            'classification_breaks': classification_breaks,
            'saved_files': saved_files
        }

    except Exception as e:
        st.error(f"è®¡ç®—å¤±è´¥: {e}")
        import traceback
        st.error(traceback.format_exc())
        return None


def main():
    # æ˜¾ç¤ºå­—ä½“ä¿¡æ¯
    with st.sidebar:
        with st.expander("ğŸ”§ ç³»ç»Ÿä¿¡æ¯"):
            st.write(f"**æ“ä½œç³»ç»Ÿ:** {platform.system()}")
            current_font = plt.rcParams['font.sans-serif'][0]
            st.write(f"**ä½¿ç”¨å­—ä½“:** {current_font}")

            st.markdown("---")
            st.markdown("""
            **è§£å†³ä¸­æ–‡ä¹±ç :**

            å¦‚æœä»æœ‰ä¹±ç :
            1. ç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½å­—ä½“
            2. æˆ–æ‰‹åŠ¨å®‰è£…ä¸­æ–‡å­—ä½“

            Linux:
            ```
            sudo apt install fonts-wqy-microhei
            ```
            """)

    st.title("ğŸŒ¿ RSEIè®¡ç®—ç³»ç»Ÿ v3.8 - å½»åº•ä¿®å¤ä¸­æ–‡")
    st.markdown("**Remote Sensing based Ecological Index é¥æ„Ÿç”Ÿæ€æŒ‡æ•°è®¡ç®—å·¥å…·**")

    # å…¶ä½™ç•Œé¢ä»£ç ä¿æŒä¸å˜...
    with st.sidebar:
        st.header("âš™ï¸ å‚æ•°é…ç½®")
        st.subheader("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
        uploaded_file = st.file_uploader("é€‰æ‹©å¤šæ³¢æ®µTIFå½±åƒ", type=['tif', 'tiff'])

        st.subheader("ğŸ›°ï¸ å«æ˜Ÿå‚æ•°")
        satellite = st.selectbox("å«æ˜Ÿç±»å‹", ["Landsat8", "Sentinel2"], index=0)

        st.subheader("ğŸ”¬ è®¡ç®—æ–¹æ³•")
        use_pca = st.checkbox("ä½¿ç”¨PCAæ–¹æ³•", value=True)

        st.subheader("ğŸ“Š åˆ†ç±»é˜ˆå€¼è®¾ç½®")
        use_jenks = st.checkbox("ä½¿ç”¨Jenksè‡ªç„¶é—´æ–­ç‚¹", value=True)

        if use_jenks:
            jenks_samples = st.slider("é‡‡æ ·æ•°é‡", 1000, 20000, 5000, 1000)
        else:
            st.write("æ‰‹åŠ¨è®¾ç½®é˜ˆå€¼:")
            threshold_1 = st.number_input("å·®/è¾ƒå·®", 0.0, 1.0, 0.2, 0.01)
            threshold_2 = st.number_input("è¾ƒå·®/ä¸­ç­‰", 0.0, 1.0, 0.4, 0.01)
            threshold_3 = st.number_input("ä¸­ç­‰/è‰¯å¥½", 0.0, 1.0, 0.6, 0.01)
            threshold_4 = st.number_input("è‰¯å¥½/ä¼˜ç§€", 0.0, 1.0, 0.8, 0.01)

        st.subheader("ğŸ’§ æ°´ä½“æ©è†œ")
        mask_water = st.checkbox("å»é™¤æ°´åŸŸ", value=True)

        if mask_water:
            water_index = st.selectbox("æ°´ä½“æŒ‡æ•°", ["MNDWI", "NDWI", "AWEIsh"], index=0)
            use_otsu = st.checkbox("ä½¿ç”¨OTSUè‡ªåŠ¨è®¡ç®—é˜ˆå€¼", value=True)
            if not use_otsu:
                water_threshold = st.number_input("æ‰‹åŠ¨é˜ˆå€¼", -1.0, 1.0, 0.0, 0.1)
            else:
                water_threshold = None
        else:
            water_index = "MNDWI"
            use_otsu = True
            water_threshold = None

        st.subheader("ğŸ’¾ å¯¼å‡ºé€‰é¡¹")
        export_geotiff = st.checkbox("å¯¼å‡ºGeoTIFFæ–‡ä»¶", value=True)
        export_indices = st.checkbox("å¯¼å‡ºæ‰€æœ‰é¥æ„ŸæŒ‡æ•°", value=True)

    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.tif') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        st.success(f"âœ… æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")
        file_size = len(uploaded_file.getvalue()) / (1024 * 1024)
        st.info(f"ğŸ“¦ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

        if st.button("â–¶ï¸ å¼€å§‹è®¡ç®—", type="primary"):
            if not use_jenks:
                thresholds = [threshold_1, threshold_2, threshold_3, threshold_4]
                if not all(thresholds[i] < thresholds[i + 1] for i in range(3)):
                    st.error("âŒ é˜ˆå€¼å¿…é¡»é€’å¢ï¼")
                    return

            config = RSEIConfig(
                satellite=satellite,
                use_pca=use_pca,
                export_indices=export_indices,
                export_geotiff=export_geotiff,
                mask_water=mask_water,
                water_index=water_index,
                water_threshold=water_threshold,
                use_otsu=use_otsu,
                use_jenks=use_jenks,
                classification_breaks=[threshold_1, threshold_2, threshold_3,
                                       threshold_4] if not use_jenks else [0.2, 0.4, 0.6, 0.8],
                jenks_samples=jenks_samples if use_jenks else 5000
            )

            start_time = time.time()

            with st.spinner("è®¡ç®—ä¸­ï¼Œè¯·ç¨å€™..."):
                results = execute_rsei_calculation(tmp_file_path, config)

            elapsed_time = time.time() - start_time

            if results:
                st.success(f"âœ… è®¡ç®—å®Œæˆï¼è€—æ—¶: {elapsed_time:.1f}ç§’")
                st.header("ğŸ“Š è®¡ç®—ç»“æœ")

                tab1, tab2, tab3, tab4, tab5 = st.tabs([
                    "ğŸ“ˆ ç»Ÿè®¡æ•°æ®",
                    "ğŸ–¼ï¸ å¯è§†åŒ–ç»“æœ",
                    "ğŸ“¥ ä¸‹è½½æ–‡ä»¶",
                    "ğŸ“‹ æ–‡ä»¶æ¸…å•",
                    "â„¹ï¸ è¯¦ç»†ä¿¡æ¯"
                ])

                with tab1:
                    st.subheader("æŒ‡æ ‡ç»Ÿè®¡")
                    st.dataframe(results['stats_df'], use_container_width=True)
                    st.subheader("ç­‰çº§åˆ†å¸ƒ")
                    st.dataframe(results['class_df'], use_container_width=True)
                    st.subheader("åˆ†ç±»é˜ˆå€¼")
                    st.dataframe(results['threshold_df'], use_container_width=True)

                with tab2:
                    st.subheader("RSEIç»¼åˆåˆ†æå¯è§†åŒ–")
                    st.image(results['img_path'], use_column_width=True)
                    st.success("âœ… ä¸­æ–‡æ˜¾ç¤ºå·²ä¿®å¤ï¼å¦‚ä»æœ‰é—®é¢˜è¯·å®‰è£…ä¸­æ–‡å­—ä½“ã€‚")

                with tab3:
                    st.subheader("ä¸‹è½½ç»“æœæ–‡ä»¶")
                    col1, col2 = st.columns(2)

                    with col1:
                        with open(results['img_path'], 'rb') as f:
                            st.download_button(
                                "ğŸ“· ä¸‹è½½å¯è§†åŒ–å›¾",
                                f,
                                "RSEI_comprehensive.png",
                                "image/png",
                                use_container_width=True
                            )

                    with col2:
                        with open(results['excel_path'], 'rb') as f:
                            st.download_button(
                                "ğŸ“Š ä¸‹è½½ç»Ÿè®¡æŠ¥å‘Š",
                                f,
                                "RSEI_analysis.xlsx",
                                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                use_container_width=True
                            )

                    st.markdown("---")

                    if export_geotiff:
                        st.subheader("ğŸ“¦ æ‰“åŒ…ä¸‹è½½")
                        zip_buffer = io.BytesIO()
                        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                            output_path = results['output_path']
                            for file in output_path.glob('*'):
                                zip_file.write(file, file.name)

                            readme = """RSEIè®¡ç®—ç»“æœ

åŒ…å«æ‰€æœ‰è®¡ç®—ç»“æœå’Œé¥æ„ŸæŒ‡æ•°TIFæ–‡ä»¶ã€‚
è¯¦è§RSEI_analysis.xlsxæ–‡ä»¶æ¸…å•ã€‚"""
                            zip_file.writestr('README.txt', readme.encode('utf-8'))

                        zip_size = len(zip_buffer.getvalue()) / (1024 * 1024)

                        st.download_button(
                            f"ğŸ“¦ ä¸‹è½½æ‰€æœ‰ç»“æœ - {zip_size:.2f} MB",
                            zip_buffer.getvalue(),
                            f"RSEI_results_{time.strftime('%Y%m%d_%H%M%S')}.zip",
                            "application/zip",
                            use_container_width=True
                        )

                with tab4:
                    st.subheader("è¾“å‡ºæ–‡ä»¶æ¸…å•")
                    st.dataframe(results['files_df'], use_container_width=True)

                with tab5:
                    st.subheader("è®¡ç®—è¯¦æƒ…")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("å«æ˜Ÿç±»å‹", config.satellite)
                        st.metric("æ€»è€—æ—¶", f"{elapsed_time:.1f}ç§’")
                    with col2:
                        st.metric("åˆ†ç±»æ–¹æ³•", 'Jenks' if config.use_jenks else 'æ‰‹åŠ¨')
                        st.metric("æ–‡ä»¶æ•°", len(results['saved_files']))

    else:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ å¤šæ³¢æ®µTIFå½±åƒ")


if __name__ == "__main__":
    main()